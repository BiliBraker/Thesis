{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arxiv tokenization test\n",
    "\n",
    "import nltk.corpus  \n",
    "from nltk import word_tokenize\n",
    "\n",
    "path = 'c:/Users/soirk/Krisztian/Egyetem/Survey Statisztika Msc/Szakdolgozat/arxiv_txts/'\n",
    "\n",
    "paper = open(path+'0.00094.txt','r')\n",
    "\n",
    "paper_token=word_tokenize(paper.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imputation\n",
      "imput\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "ps = PorterStemmer() \n",
    "\n",
    "print(wnl.lemmatize('imputation'))\n",
    "print(ps.stem('imputed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnvariable', 'selection', 'additive', 'partial', 'linear', 'quantile', 'regression', 'missing', 'covariates', 'nnvariable', 'selection', 'additive', 'partial', 'linear', 'quantile', 'regressionnnwith', 'missing', 'covariatesnnben', 'sherwoodnnjohns', 'hopkins', 'universitynnabstract', 'standard', 'quantile', 'regression', 'model', 'assumes', 'linear', 'relationship', 'atnnthe', 'quantile', 'interest', 'variables', 'observed', 'assumptions', 'arennrelaxed', 'considering', 'partial', 'linear', 'model', 'missing', 'covariates', 'weightednnobjective', 'function', 'using', 'inverse', 'probability', 'weighting', 'used', 'remove', 'thennpotential', 'bias', 'caused', 'missing', 'data', 'estimators', 'using', 'parametric', 'nonpara', 'nnmetric', 'estimates', 'probability', 'observation', 'fully', 'observed', 'covariatesnnare', 'examined', 'penalized', 'weighted', 'objective', 'function', 'using', 'nonconvexnnpenalties', 'mcp', 'scad', 'used', 'variable', 'selection', 'linear', 'terms', 'thennpresence', 'missing', 'data', 'assuming', 'missing', 'data', 'problems', 'remains', 'low', 'di', 'nnmensional', 'problem', 'penalized', 'estimator', 'oracle', 'property', 'including', 'casesnnwhere', 'p', 'n']\n",
      "13423 10145\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in paper_token if not w in stop_words]\n",
    "print(words[:100])\n",
    "\n",
    "print(len(paper_token), len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'selection',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'nnvariable',\n",
       " 'selection',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regressionnnwith',\n",
       " 'missing',\n",
       " 'covariatesnnben',\n",
       " 'sherwoodnnjohns',\n",
       " 'hopkins',\n",
       " 'universitynnabstract',\n",
       " 'standard',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'model',\n",
       " 'assumes',\n",
       " 'linear',\n",
       " 'relationship',\n",
       " 'atnnthe',\n",
       " 'quantile',\n",
       " 'interest',\n",
       " 'variables',\n",
       " 'observed',\n",
       " 'assumptions',\n",
       " 'arennrelaxed',\n",
       " 'considering',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'model',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'weightednnobjective',\n",
       " 'function',\n",
       " 'using',\n",
       " 'inverse',\n",
       " 'probability',\n",
       " 'weighting',\n",
       " 'used',\n",
       " 'remove',\n",
       " 'thennpotential',\n",
       " 'bias',\n",
       " 'caused',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'estimators',\n",
       " 'using',\n",
       " 'parametric',\n",
       " 'nonpara',\n",
       " 'nnmetric',\n",
       " 'estimates',\n",
       " 'probability',\n",
       " 'observation',\n",
       " 'fully',\n",
       " 'observed',\n",
       " 'covariatesnnare',\n",
       " 'examined',\n",
       " 'penalized',\n",
       " 'weighted',\n",
       " 'objective',\n",
       " 'function',\n",
       " 'using',\n",
       " 'nonconvexnnpenalties',\n",
       " 'mcp',\n",
       " 'scad',\n",
       " 'used',\n",
       " 'variable',\n",
       " 'selection',\n",
       " 'linear',\n",
       " 'terms',\n",
       " 'thennpresence',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'assuming',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'problems',\n",
       " 'remains',\n",
       " 'low',\n",
       " 'di',\n",
       " 'nnmensional',\n",
       " 'problem',\n",
       " 'penalized',\n",
       " 'estimator',\n",
       " 'oracle',\n",
       " 'property',\n",
       " 'including',\n",
       " 'casesnnwhere',\n",
       " 'p',\n",
       " 'n',\n",
       " 'theoretical',\n",
       " 'challenges',\n",
       " 'include',\n",
       " 'handling',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'partial',\n",
       " 'lin',\n",
       " 'nnear',\n",
       " 'models',\n",
       " 'working',\n",
       " 'nonsmooth',\n",
       " 'loss',\n",
       " 'function',\n",
       " 'penaltynnfunction',\n",
       " 'performance',\n",
       " 'method',\n",
       " 'evaluated',\n",
       " 'using',\n",
       " 'monte',\n",
       " 'carlo',\n",
       " 'simu',\n",
       " 'nnlations',\n",
       " 'methods',\n",
       " 'applied',\n",
       " 'model',\n",
       " 'amount',\n",
       " 'time',\n",
       " 'sober',\n",
       " 'patientsnnleaving',\n",
       " 'rehabilitation',\n",
       " 'center',\n",
       " 'nnkey',\n",
       " 'words',\n",
       " 'phrases',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'inversennprobability',\n",
       " 'weighting',\n",
       " 'variable',\n",
       " 'selection',\n",
       " 'scad',\n",
       " 'mcp',\n",
       " 'nn',\n",
       " 'introductionnnlinear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'proposed',\n",
       " 'koenker',\n",
       " 'bassett',\n",
       " 'pro',\n",
       " 'nnvides',\n",
       " 'estimate',\n",
       " 'conditional',\n",
       " 'quantile',\n",
       " 'without',\n",
       " 'requiring',\n",
       " 'distributionalnnassumptions',\n",
       " 'assumes',\n",
       " 'linear',\n",
       " 'relationship',\n",
       " 'response',\n",
       " 'co',\n",
       " 'nnvariates',\n",
       " 'quantile',\n",
       " 'interest',\n",
       " 'variables',\n",
       " 'fully',\n",
       " 'observed',\n",
       " 'thisnnpaper',\n",
       " 'introduces',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'model',\n",
       " 'accommodate',\n",
       " 'missingnncovariates',\n",
       " 'inverse',\n",
       " 'probability',\n",
       " 'weighting',\n",
       " 'ipw',\n",
       " 'used',\n",
       " 'remove',\n",
       " 'potential',\n",
       " 'biasnncaused',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'ipw',\n",
       " 'framework',\n",
       " 'fits',\n",
       " 'nicely',\n",
       " 'quantile',\n",
       " 'regressionnnbecause',\n",
       " 'require',\n",
       " 'distributional',\n",
       " 'assumptions',\n",
       " 'covariates',\n",
       " 'ornnarnxnnivn',\n",
       " 'nn',\n",
       " 'n',\n",
       " 'nn',\n",
       " 'n',\n",
       " 'nn',\n",
       " 'vn',\n",
       " 'nn',\n",
       " 'nstnnatn',\n",
       " 'mnnen',\n",
       " 'nn',\n",
       " 'n',\n",
       " 'jnnunn',\n",
       " 'nn',\n",
       " 'n',\n",
       " 'nnnnvariable',\n",
       " 'selection',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'nnresponse',\n",
       " 'however',\n",
       " 'require',\n",
       " 'model',\n",
       " 'probability',\n",
       " 'observationnnhas',\n",
       " 'complete',\n",
       " 'data',\n",
       " 'provide',\n",
       " 'flexibility',\n",
       " 'probabilities',\n",
       " 'estimated',\n",
       " 'us',\n",
       " 'nning',\n",
       " 'parametric',\n",
       " 'nonparametric',\n",
       " 'methods',\n",
       " 'weighted',\n",
       " 'penalized',\n",
       " 'objectivennfunction',\n",
       " 'proposed',\n",
       " 'variable',\n",
       " 'selection',\n",
       " 'linear',\n",
       " 'covariates',\n",
       " 'presencennof',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'nnconsider',\n",
       " 'sample',\n",
       " 'yi',\n",
       " 'zi',\n",
       " 'ni',\n",
       " 'yi',\n",
       " 'r',\n",
       " 'rpnnand',\n",
       " 'zi',\n",
       " 'zi',\n",
       " 'zid',\n",
       " 'n',\n",
       " 'rd',\n",
       " 'conditional',\n",
       " 'quantile',\n",
       " 'z',\n",
       " 'defined',\n",
       " 'z',\n",
       " 'pr',\n",
       " 'z',\n",
       " 'z',\n",
       " 'papernconsiders',\n",
       " 'model',\n",
       " 'given',\n",
       " 'value',\n",
       " 'linear',\n",
       " 'relationshipnwith',\n",
       " 'unknown',\n",
       " 'relationship',\n",
       " 'z',\n",
       " 'modelnnyi',\n",
       " 'ni',\n",
       " 'g',\n",
       " 'zi',\n",
       " 'nnwhere',\n",
       " 'pr',\n",
       " 'zi',\n",
       " 'g',\n",
       " 'zi',\n",
       " 'nxe',\n",
       " 'dnnj',\n",
       " 'gj',\n",
       " 'zij',\n",
       " 'observ',\n",
       " 'nnable',\n",
       " 'variables',\n",
       " 'yi',\n",
       " 'zi',\n",
       " 'assumed',\n",
       " 'marginal',\n",
       " 'stillnnallows',\n",
       " 'common',\n",
       " 'cases',\n",
       " 'zi',\n",
       " 'n',\n",
       " 'recent',\n",
       " 'work',\n",
       " 'estimat',\n",
       " 'ning',\n",
       " 'conditional',\n",
       " 'quantiles',\n",
       " 'presence',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'chen',\n",
       " 'wan',\n",
       " 'zhounn',\n",
       " 'made',\n",
       " 'major',\n",
       " 'distinction',\n",
       " 'n',\n",
       " 'errors',\n",
       " 'section',\n",
       " 'nnincludes',\n",
       " 'discussion',\n",
       " 'philosophy',\n",
       " 'marginal',\n",
       " 'assumption',\n",
       " 'isnnreasonable',\n",
       " 'accommodate',\n",
       " 'cases',\n",
       " 'n',\n",
       " 'errors',\n",
       " 'model',\n",
       " 'identifiabilitynnit',\n",
       " 'assumed',\n",
       " 'e',\n",
       " 'gj',\n",
       " 'zij',\n",
       " 'j',\n",
       " 'intercept',\n",
       " 'part',\n",
       " 'unknownnnonlinear',\n",
       " 'function',\n",
       " 'technical',\n",
       " 'simplicity',\n",
       " 'assumed',\n",
       " 'e',\n",
       " 'thenadditive',\n",
       " 'model',\n",
       " 'z',\n",
       " 'allows',\n",
       " 'nonlinear',\n",
       " 'functions',\n",
       " 'avoiding',\n",
       " 'cthe',\n",
       " 'cursennof',\n",
       " 'dimensionalityxe',\n",
       " 'intercept',\n",
       " 'linear',\n",
       " 'coefficients',\n",
       " 'nonlinearnnfunction',\n",
       " 'g',\n",
       " 'z',\n",
       " 'depend',\n",
       " 'ease',\n",
       " 'notation',\n",
       " 'dropped',\n",
       " 'thennrest',\n",
       " 'article',\n",
       " 'without',\n",
       " 'loss',\n",
       " 'generality',\n",
       " 'first',\n",
       " 'q',\n",
       " 'coefficients',\n",
       " 'arennnonzero',\n",
       " 'remaining',\n",
       " 'pxe',\n",
       " 'q',\n",
       " 'coefficients',\n",
       " 'zero',\n",
       " 'formally',\n",
       " 'pxe',\n",
       " 'q',\n",
       " 'nnwith',\n",
       " 'rq',\n",
       " 'pxe',\n",
       " 'q',\n",
       " 'rpxe',\n",
       " 'q',\n",
       " 'let',\n",
       " 'n',\n",
       " 'p',\n",
       " 'linear',\n",
       " 'covariates',\n",
       " 'nn',\n",
       " 'nxc',\n",
       " 'q',\n",
       " 'ofnthe',\n",
       " 'active',\n",
       " 'linear',\n",
       " 'covariates',\n",
       " 'corresponding',\n",
       " 'first',\n",
       " 'q',\n",
       " 'columns',\n",
       " 'casennof',\n",
       " 'fixed',\n",
       " 'increasing',\n",
       " 'q',\n",
       " 'p',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'increasingnncase',\n",
       " 'covariates',\n",
       " 'indexed',\n",
       " 'qn',\n",
       " 'pn',\n",
       " 'nnthis',\n",
       " 'paper',\n",
       " 'addresses',\n",
       " 'estimating',\n",
       " 'subset',\n",
       " 'zi',\n",
       " 'values',\n",
       " 'thatnare',\n",
       " 'always',\n",
       " 'observed',\n",
       " 'missing',\n",
       " 'random',\n",
       " 'assumption',\n",
       " 'holds',\n",
       " 'robinsnnet',\n",
       " 'al',\n",
       " 'proposed',\n",
       " 'handling',\n",
       " 'potential',\n",
       " 'bias',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'usingnnnnvariable',\n",
       " 'selection',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'nna',\n",
       " 'weighted',\n",
       " 'estimating',\n",
       " 'equation',\n",
       " 'estimates',\n",
       " 'derived',\n",
       " 'using',\n",
       " 'observations',\n",
       " 'withnncomplete',\n",
       " 'data',\n",
       " 'weights',\n",
       " 'account',\n",
       " 'missing',\n",
       " 'observations',\n",
       " 'weightsnnare',\n",
       " 'assigned',\n",
       " 'using',\n",
       " 'inverse',\n",
       " 'probability',\n",
       " 'weighting',\n",
       " 'ipw',\n",
       " 'weight',\n",
       " 'fornnan',\n",
       " 'observation',\n",
       " 'inverse',\n",
       " 'probability',\n",
       " 'observation',\n",
       " 'samennobserved',\n",
       " 'variables',\n",
       " 'complete',\n",
       " 'data',\n",
       " 'thus',\n",
       " 'observations',\n",
       " 'similar',\n",
       " 'withnnmissing',\n",
       " 'data',\n",
       " 'receive',\n",
       " 'larger',\n",
       " 'weights',\n",
       " 'nnwang',\n",
       " 'wang',\n",
       " 'gutierrez',\n",
       " 'carroll',\n",
       " 'consider',\n",
       " 'using',\n",
       " 'weighted',\n",
       " 'ap',\n",
       " 'nnproach',\n",
       " 'local',\n",
       " 'linear',\n",
       " 'smoother',\n",
       " 'generalized',\n",
       " 'univariate',\n",
       " 'linear',\n",
       " 'modelnnwith',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'liang',\n",
       " 'et',\n",
       " 'al',\n",
       " 'applied',\n",
       " 'ipw',\n",
       " 'approach',\n",
       " 'partialnnlinear',\n",
       " 'mean',\n",
       " 'models',\n",
       " 'assumed',\n",
       " 'nonlinear',\n",
       " 'covariates',\n",
       " 'always',\n",
       " 'observed',\n",
       " 'nnlipsitz',\n",
       " 'et',\n",
       " 'al',\n",
       " 'yi',\n",
       " 'proposed',\n",
       " 'ipw',\n",
       " 'methods',\n",
       " 'longitu',\n",
       " 'nndinal',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'models',\n",
       " 'dropouts',\n",
       " 'sherwood',\n",
       " 'et',\n",
       " 'al',\n",
       " 'used',\n",
       " 'thennipw',\n",
       " 'approach',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'proposed',\n",
       " 'bic',\n",
       " 'type',\n",
       " 'procedurennwith',\n",
       " 'weighted',\n",
       " 'objective',\n",
       " 'function',\n",
       " 'model',\n",
       " 'selection',\n",
       " 'liu',\n",
       " 'yuan',\n",
       " 'pro',\n",
       " 'nnposed',\n",
       " 'weighted',\n",
       " 'empirical',\n",
       " 'likelihood',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'estimator',\n",
       " 'missingnncovariates',\n",
       " 'achieves',\n",
       " 'semiparametric',\n",
       " 'efficiency',\n",
       " 'wei',\n",
       " 'et',\n",
       " 'al',\n",
       " 'presented',\n",
       " 'annmultiple',\n",
       " 'imputation',\n",
       " 'solution',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'assume',\n",
       " 'lin',\n",
       " 'nnear',\n",
       " 'model',\n",
       " 'holds',\n",
       " 'quantiles',\n",
       " 'address',\n",
       " 'efficiency',\n",
       " 'loss',\n",
       " 'bias',\n",
       " 'causednnby',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'wei',\n",
       " 'yang',\n",
       " 'use',\n",
       " 'multiple',\n",
       " 'imputation',\n",
       " 'handlennbias',\n",
       " 'caused',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'assume',\n",
       " 'annlocation',\n",
       " 'scale',\n",
       " 'model',\n",
       " 'limit',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'influence',\n",
       " 'thennlocation',\n",
       " 'chen',\n",
       " 'wan',\n",
       " 'zhou',\n",
       " 'provide',\n",
       " 'thorough',\n",
       " 'analysis',\n",
       " 'efficiencynnusing',\n",
       " 'ipw',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'propose',\n",
       " 'three',\n",
       " 'different',\n",
       " 'estimat',\n",
       " 'nning',\n",
       " 'equations',\n",
       " 'using',\n",
       " 'ipw',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'nnthey',\n",
       " 'estimate',\n",
       " 'weights',\n",
       " 'nonparametrically',\n",
       " 'demonstrate',\n",
       " 'estima',\n",
       " 'nntors',\n",
       " 'achieve',\n",
       " 'semiparametric',\n",
       " 'efficiency',\n",
       " 'bound',\n",
       " 'propose',\n",
       " 'estimatingnnequation',\n",
       " 'approach',\n",
       " 'focused',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'methodsnnin',\n",
       " 'paper',\n",
       " 'work',\n",
       " 'directly',\n",
       " 'objective',\n",
       " 'function',\n",
       " 'relaxes',\n",
       " 'linearitynnassumption',\n",
       " 'nnhe',\n",
       " 'shi',\n",
       " 'proposed',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'model',\n",
       " 'usingnnb',\n",
       " 'splines',\n",
       " 'assume',\n",
       " 'additive',\n",
       " 'structure',\n",
       " 'limited',\n",
       " 'modelnnto',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'extended',\n",
       " 'variety',\n",
       " 'ofnnsettings',\n",
       " 'including',\n",
       " 'longitudinal',\n",
       " 'models',\n",
       " 'et',\n",
       " 'al',\n",
       " 'varyingnnnnvariable',\n",
       " 'selection',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'nncoefficient',\n",
       " 'models',\n",
       " 'wang',\n",
       " 'et',\n",
       " 'al',\n",
       " 'p',\n",
       " 'unknown',\n",
       " 'functions',\n",
       " 'arennestimated',\n",
       " 'horowitz',\n",
       " 'lee',\n",
       " 'de',\n",
       " 'gooijer',\n",
       " 'zerom',\n",
       " 'proposednnfully',\n",
       " 'nonparametric',\n",
       " 'additive',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'models',\n",
       " 'nnthis',\n",
       " 'paper',\n",
       " 'provides',\n",
       " 'several',\n",
       " 'new',\n",
       " 'contributions',\n",
       " 'first',\n",
       " 'ipw',\n",
       " 'extended',\n",
       " 'thennpartial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'model',\n",
       " 'second',\n",
       " 'penalized',\n",
       " 'weighted',\n",
       " 'objec',\n",
       " 'nntive',\n",
       " 'function',\n",
       " 'simultaneous',\n",
       " 'estimation',\n",
       " 'variable',\n",
       " 'selection',\n",
       " 'presence',\n",
       " 'ofnnmissing',\n",
       " 'data',\n",
       " 'proposed',\n",
       " 'paper',\n",
       " 'includes',\n",
       " 'discussion',\n",
       " 'reasonablennto',\n",
       " 'assume',\n",
       " 'variables',\n",
       " 'presence',\n",
       " 'conditional',\n",
       " 'n',\n",
       " 'errors',\n",
       " 'addition',\n",
       " 'nnit',\n",
       " 'shown',\n",
       " 'model',\n",
       " 'selection',\n",
       " 'consistency',\n",
       " 'holds',\n",
       " 'high',\n",
       " 'dimensional',\n",
       " 'casennof',\n",
       " 'p',\n",
       " 'n',\n",
       " 'techniques',\n",
       " 'used',\n",
       " 'density',\n",
       " 'estimation',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'miss',\n",
       " 'nning',\n",
       " 'data',\n",
       " 'nonlinear',\n",
       " 'estimation',\n",
       " 'penalties',\n",
       " 'combined',\n",
       " 'addressnnthese',\n",
       " 'problems',\n",
       " 'nnfirst',\n",
       " 'asymptotic',\n",
       " 'behaviors',\n",
       " 'oracle',\n",
       " 'model',\n",
       " 'analyzed',\n",
       " 'thennpartial',\n",
       " 'linear',\n",
       " 'model',\n",
       " 'includes',\n",
       " 'column',\n",
       " 'vectors',\n",
       " 'linear',\n",
       " 'covari',\n",
       " 'nnates',\n",
       " 'next',\n",
       " 'simultaneous',\n",
       " 'estimation',\n",
       " 'variable',\n",
       " 'selection',\n",
       " 'potentialnncovariates',\n",
       " 'considered',\n",
       " 'adding',\n",
       " 'penalty',\n",
       " 'function',\n",
       " 'weightednnobjective',\n",
       " 'function',\n",
       " 'work',\n",
       " 'assumed',\n",
       " 'nonlinear',\n",
       " 'terms',\n",
       " 'knownnna',\n",
       " 'priori',\n",
       " 'part',\n",
       " 'true',\n",
       " 'model',\n",
       " 'restrict',\n",
       " 'variable',\n",
       " 'selection',\n",
       " 'linearnnterms',\n",
       " 'however',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'section',\n",
       " 'weighted',\n",
       " 'bic',\n",
       " 'approach',\n",
       " 'isnnproposed',\n",
       " 'designate',\n",
       " 'variables',\n",
       " 'linear',\n",
       " 'nonlinear',\n",
       " 'penalized',\n",
       " 'objectivennfunction',\n",
       " 'uses',\n",
       " 'penalties',\n",
       " 'either',\n",
       " 'scad',\n",
       " 'penalty',\n",
       " 'fan',\n",
       " 'li',\n",
       " 'nnor',\n",
       " 'mcp',\n",
       " 'zhang',\n",
       " 'standard',\n",
       " 'conditions',\n",
       " 'penalized',\n",
       " 'estimatornnhas',\n",
       " 'oracle',\n",
       " 'property',\n",
       " 'set',\n",
       " 'local',\n",
       " 'minimums',\n",
       " 'nonconvexnnobjective',\n",
       " 'function',\n",
       " 'exists',\n",
       " 'estimator',\n",
       " 'asymptotically',\n",
       " 'efficient',\n",
       " 'asnnif',\n",
       " 'true',\n",
       " 'linear',\n",
       " 'covariates',\n",
       " 'known',\n",
       " 'priori',\n",
       " 'liu',\n",
       " 'et',\n",
       " 'al',\n",
       " 'used',\n",
       " 'thennscad',\n",
       " 'penalty',\n",
       " 'select',\n",
       " 'linear',\n",
       " 'components',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linearnnmean',\n",
       " 'regression',\n",
       " 'model',\n",
       " 'wu',\n",
       " 'liu',\n",
       " 'demonstrated',\n",
       " 'linear',\n",
       " 'quan',\n",
       " 'nntile',\n",
       " 'regression',\n",
       " 'estimator',\n",
       " 'minimizing',\n",
       " 'penalized',\n",
       " 'objective',\n",
       " 'function',\n",
       " 'withnnthe',\n",
       " 'scad',\n",
       " 'penalty',\n",
       " 'oracle',\n",
       " 'property',\n",
       " 'use',\n",
       " 'penalties',\n",
       " 'fornnp',\n",
       " 'n',\n",
       " 'explored',\n",
       " 'linear',\n",
       " 'wang',\n",
       " 'et',\n",
       " 'al',\n",
       " 'additive',\n",
       " 'partialnnlinear',\n",
       " 'sherwood',\n",
       " 'wang',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'nnthe',\n",
       " 'article',\n",
       " 'structured',\n",
       " 'follows',\n",
       " 'section',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linearnnquantile',\n",
       " 'regression',\n",
       " 'model',\n",
       " 'missing',\n",
       " 'linear',\n",
       " 'covariates',\n",
       " 'introduced',\n",
       " 'selection',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " 'regression',\n",
       " 'missing',\n",
       " 'covariates',\n",
       " 'nn',\n",
       " 'focuses',\n",
       " 'weighted',\n",
       " 'penalized',\n",
       " 'objective',\n",
       " 'function',\n",
       " 'model',\n",
       " 'selection',\n",
       " 'ofnnlinear',\n",
       " 'covariates',\n",
       " 'presence',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'size',\n",
       " 'perfor',\n",
       " 'nnmance',\n",
       " 'penalized',\n",
       " 'estimator',\n",
       " 'analyzed',\n",
       " 'using',\n",
       " 'monte',\n",
       " 'carlo',\n",
       " 'simulations',\n",
       " 'innnsection',\n",
       " 'section',\n",
       " 'proposed',\n",
       " 'methods',\n",
       " 'used',\n",
       " 'model',\n",
       " 'amount',\n",
       " 'timennsober',\n",
       " 'patients',\n",
       " 'rehabilitation',\n",
       " 'center',\n",
       " 'addition',\n",
       " 'section',\n",
       " 'includesnna',\n",
       " 'proposal',\n",
       " 'bic',\n",
       " 'type',\n",
       " 'procedure',\n",
       " 'designate',\n",
       " 'whether',\n",
       " 'variable',\n",
       " 'linearnnor',\n",
       " 'nonlinear',\n",
       " 'section',\n",
       " 'concludes',\n",
       " 'paper',\n",
       " 'summary',\n",
       " 'discussion',\n",
       " 'ofnndirections',\n",
       " 'future',\n",
       " 'research',\n",
       " 'nn',\n",
       " 'inverse',\n",
       " 'probability',\n",
       " 'weightingnn',\n",
       " 'additive',\n",
       " 'partial',\n",
       " 'linear',\n",
       " 'quantile',\n",
       " ...]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_c = [w for w in words if not w.startswith('x') and not w.endswith('x') and 'nnnnn' not in w]\n",
    "\n",
    "print(len(words_c))\n",
    "words_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6938 10145\n",
      "n\n",
      "nn\n",
      "zi\n",
      "b\n",
      "nxe\n",
      "ni\n",
      "j\n",
      "z\n",
      "p\n",
      "k\n",
      "ti\n",
      "w\n",
      "r\n",
      "c\n",
      "yi\n",
      "g\n",
      "wt\n",
      "e\n",
      "let\n",
      "qn\n",
      "l\n",
      "al\n",
      "zik\n",
      "ri\n",
      "pn\n",
      "et\n",
      "h\n",
      "u\n",
      "mcp\n",
      "ipw\n",
      "ann\n",
      "q\n",
      "enn\n",
      "wan\n",
      "use\n",
      "li\n",
      "opn\n",
      "hsn\n",
      "jn\n",
      "ui\n",
      "pr\n",
      "set\n",
      "one\n",
      "kn\n",
      "ln\n",
      "nne\n",
      "jxe\n",
      "op\n",
      "zij\n",
      "bic\n",
      "non\n",
      "nhb\n",
      "lnn\n",
      "gj\n",
      "pxe\n",
      "liu\n",
      "new\n",
      "two\n",
      "arg\n",
      "fi\n",
      "inn\n",
      "fit\n",
      "rn\n",
      "mse\n",
      "zid\n",
      "dln\n",
      "dxe\n",
      "may\n",
      "npn\n",
      "np\n",
      "tv\n",
      "fv\n",
      "nti\n",
      "low\n",
      "nna\n",
      "wei\n",
      "v\n",
      "zxe\n",
      "zn\n",
      "mi\n",
      "nkn\n",
      "hdr\n",
      "nfi\n",
      "f\n",
      "nri\n",
      "nj\n",
      "ntn\n",
      "qi\n",
      "lee\n",
      "wu\n",
      "hr\n",
      "bj\n",
      "ne\n",
      "hj\n",
      "hxe\n",
      "ij\n",
      "th\n",
      "nh\n",
      "en\n",
      "nen\n",
      "see\n",
      "nnp\n",
      "nk\n",
      "nnw\n",
      "nw\n",
      "rqn\n",
      "wxe\n",
      "uni\n",
      "ai\n",
      "mn\n",
      "lin\n",
      "pro\n",
      "co\n",
      "us\n",
      "nxc\n",
      "shi\n",
      "fan\n",
      "mth\n",
      "gr\n",
      "zj\n",
      "si\n",
      "bln\n",
      "rp\n",
      "rk\n",
      "mar\n",
      "nnn\n",
      "nnj\n",
      "tj\n",
      "kh\n",
      "kj\n",
      "ith\n",
      "pdf\n",
      "nb\n",
      "nab\n",
      "nbb\n",
      "gjs\n",
      "fyj\n",
      "sxe\n",
      "yu\n",
      "nyi\n",
      "qnn\n",
      "epn\n",
      "nno\n",
      "ekn\n",
      "hnb\n",
      "tao\n",
      "txe\n",
      "key\n",
      "es\n",
      "pre\n",
      "axe\n",
      "wn\n",
      "wd\n",
      "nni\n",
      "sum\n",
      "wkn\n",
      "sj\n",
      "via\n",
      "zhu\n",
      "org\n",
      "web\n",
      "soc\n",
      "di\n",
      "vn\n",
      "rd\n",
      "rq\n",
      "ap\n",
      "de\n",
      "zd\n",
      "skn\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "words_cleaned = []\n",
    "\n",
    "for w in range(len(words_c)):\n",
    "    if words_c[w].startswith('nn'):\n",
    "        clean=words_c[w].replace('nn','')\n",
    "        clean1=words_c[w-1]+clean\n",
    "        if d.check(clean1) == True:\n",
    "            words_cleaned.append(clean1)\n",
    "        if len(clean) > 0 and d.check(clean) == True:  \n",
    "            words_cleaned.append(clean)\n",
    "    if words_c[w]\n",
    "    else:\n",
    "        words_cleaned.append(w)\n",
    "\n",
    "\n",
    "fdist1 = FreqDist(words_c)\n",
    "\n",
    "keywords = [\"missing\",\"impute\",\"imputing\",\"imputation\",\"imputed\",\"pairwise\",\"listwise\",\"deletion\",\"delete\",\"deleted\"]\n",
    "\n",
    "\n",
    "print('cleaned:',len(words_cleaned),'Original:',len(words))\n",
    "\n",
    "#fdist1.most_common(50)\n",
    "\n",
    "#fdist1.plot()\n",
    "#for word in fdist1.most_common(1000):\n",
    "    #if (word[0] not in keywords) and (len(word[0]) < 4):\n",
    "       # print(word[0])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stopwords removal: 10145\n",
      "After another removal: 7192\n",
      "nnvariable\n",
      "regressionnnwith\n",
      "covariatesnnben\n",
      "sherwoodnnjohns\n",
      "universitynnabstract\n",
      "atnnthe\n",
      "arennrelaxed\n",
      "weightednnobjective\n",
      "thennpotential\n",
      "nnmetric\n",
      "covariatesnnare\n",
      "nonconvexnnpenalties\n",
      "thennpresence\n",
      "nnmensional\n",
      "casesnnwhere\n",
      "nnear\n",
      "penaltynnfunction\n",
      "nnlations\n",
      "patientsnnleaving\n",
      "nnkey\n",
      "inversennprobability\n",
      "nn\n",
      "introductionnnlinear\n",
      "nnvides\n",
      "distributionalnnassumptions\n",
      "nnvariates\n",
      "thisnnpaper\n",
      "missingnncovariates\n",
      "biasnncaused\n",
      "regressionnnbecause\n",
      "ornnarnxnnivn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nstnnatn\n",
      "mnnen\n",
      "nn\n",
      "jnnunn\n",
      "nn\n",
      "nnnnvariable\n",
      "nnresponse\n",
      "observationnnhas\n",
      "nning\n",
      "objectivennfunction\n",
      "presencennof\n",
      "nnconsider\n",
      "rpnnand\n",
      "modelnnyi\n",
      "nnwhere\n",
      "dnnj\n",
      "nnable\n",
      "stillnnallows\n",
      "zhounn\n",
      "nnincludes\n",
      "isnnreasonable\n",
      "identifiabilitynnit\n",
      "unknownnnonlinear\n",
      "cursennof\n",
      "nonlinearnnfunction\n",
      "thennrest\n",
      "arennnonzero\n",
      "nnwith\n",
      "nn\n",
      "casennof\n",
      "increasingnncase\n",
      "nnthis\n",
      "robinsnnet\n",
      "usingnnnnvariable\n",
      "nna\n",
      "withnncomplete\n",
      "weightsnnare\n",
      "fornnan\n",
      "samennobserved\n",
      "withnnmissing\n",
      "nnwang\n",
      "nnproach\n",
      "modelnnwith\n",
      "partialnnlinear\n",
      "nnlipsitz\n",
      "nndinal\n",
      "thennipw\n",
      "procedurennwith\n",
      "nnposed\n",
      "missingnncovariates\n",
      "annmultiple\n",
      "nnear\n",
      "causednnby\n",
      "handlennbias\n",
      "annlocation\n",
      "thennlocation\n",
      "efficiencynnusing\n",
      "nning\n",
      "nnthey\n",
      "nntors\n",
      "estimatingnnequation\n",
      "methodsnnin\n",
      "linearitynnassumption\n",
      "nnhe\n",
      "usingnnb\n",
      "modelnnto\n",
      "ofnnsettings\n",
      "varyingnnnnvariable\n",
      "nncoefficient\n",
      "arennestimated\n",
      "proposednnfully\n",
      "nnthis\n",
      "thennpartial\n",
      "nntive\n",
      "ofnnmissing\n",
      "reasonablennto\n",
      "nnit\n",
      "casennof\n",
      "nning\n",
      "addressnnthese\n",
      "nnfirst\n",
      "thennpartial\n",
      "nnates\n",
      "potentialnncovariates\n",
      "weightednnobjective\n",
      "knownnna\n",
      "linearnnterms\n",
      "isnnproposed\n",
      "objectivennfunction\n",
      "nnor\n",
      "estimatornnhas\n",
      "nonconvexnnobjective\n",
      "asnnif\n",
      "thennscad\n",
      "linearnnmean\n",
      "nntile\n",
      "withnnthe\n",
      "fornnp\n",
      "partialnnlinear\n",
      "nnthe\n",
      "linearnnquantile\n",
      "nn\n",
      "ofnnlinear\n",
      "nnmance\n",
      "innnsection\n",
      "timennsober\n",
      "includesnna\n",
      "linearnnor\n",
      "ofnndirections\n",
      "nn\n",
      "weightingnn\n",
      "regressionnnthis\n",
      "whichnnlinear\n",
      "fornnunderstanding\n",
      "thatnndoes\n",
      "modelnnis\n",
      "oraclennmodel\n",
      "nonconvexnnpenalized\n",
      "nnadapting\n",
      "nnsion\n",
      "ofnngenerality\n",
      "nndefinition\n",
      "anynnh\n",
      "nn\n",
      "ofnnnonlinear\n",
      "dnnj\n",
      "nnif\n",
      "nnsubintervals\n",
      "nnuniform\n",
      "nntions\n",
      "nndenote\n",
      "wherennthe\n",
      "lnnj\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nnand\n",
      "nntions\n",
      "thennintercept\n",
      "notationnand\n",
      "nnbut\n",
      "nnone\n",
      "componentsnncan\n",
      "nnxcexb\n",
      "nn\n",
      "minnxcexb\n",
      "nnwhere\n",
      "functionnnis\n",
      "nn\n",
      "covariatesnnnext\n",
      "havennmissing\n",
      "fornnthe\n",
      "innnother\n",
      "anncovariate\n",
      "missingnessnnand\n",
      "annnindicator\n",
      "fullynnobserved\n",
      "nnnnvariable\n",
      "nnwhich\n",
      "tonnbe\n",
      "missingnncan\n",
      "maynnhave\n",
      "nnpr\n",
      "nna\n",
      "tonnfit\n",
      "estimatornnis\n",
      "nnnn\n",
      "nnn\n",
      "nn\n",
      "argminn\n",
      "nnnxe\n",
      "nnwhich\n",
      "undernnthe\n",
      "nnan\n",
      "tonnalleviate\n",
      "ithnndata\n",
      "differentnnweights\n",
      "weightingnnis\n",
      "fullynnobserved\n",
      "wasnnno\n",
      "isnngiven\n",
      "observationsnnwith\n",
      "nnthe\n",
      "estimatingnnthe\n",
      "nnthe\n",
      "nnone\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nnin\n",
      "nnnnvariable\n",
      "nnthe\n",
      "linearnnmean\n",
      "regressionnnby\n",
      "estimatornnof\n",
      "nnxe\n",
      "nnj\n",
      "nnj\n",
      "nn\n",
      "nnwhere\n",
      "bandwidthnnvariable\n",
      "butnnfor\n",
      "willnndenote\n",
      "willnndenote\n",
      "willnndenote\n",
      "parametricnnweighted\n",
      "asnn\n",
      "npnn\n",
      "nn\n",
      "argminnxcexb\n",
      "nnwith\n",
      "nnpn\n",
      "nnliang\n",
      "annlocal\n",
      "nnlinear\n",
      "regressionnnestimator\n",
      "nknn\n",
      "nn\n",
      "argminnxcexb\n",
      "nn\n",
      "nnwith\n",
      "nnkn\n",
      "nn\n",
      "resultsnnto\n",
      "termsnnrequires\n",
      "hrnn\n",
      "nandnnhxe\n",
      "hdrnnnxe\n",
      "nnen\n",
      "nn\n",
      "nnnnvariable\n",
      "nnwhere\n",
      "projectionnof\n",
      "thennelement\n",
      "nn\n",
      "ofnnz\n",
      "npnn\n",
      "nnand\n",
      "nnnn\n",
      "asymptoticnndistribution\n",
      "nnficient\n",
      "nnasymptotic\n",
      "nncondition\n",
      "nnsity\n",
      "anneighborhood\n",
      "nncondition\n",
      "nnstant\n",
      "nncondition\n",
      "nncondition\n",
      "andnnxcexb\n",
      "nncondition\n",
      "nnmetric\n",
      "nnxe\n",
      "nnnnvariable\n",
      "nnwith\n",
      "functionnwith\n",
      "tnnthat\n",
      "havennbounded\n",
      "nnh\n",
      "nnsimilar\n",
      "conditonnn\n",
      "linearnnquantile\n",
      "thatnnall\n",
      "innndetail\n",
      "allowsnng\n",
      "isnnecessary\n",
      "missingnndata\n",
      "asnnthe\n",
      "arennasymptotically\n",
      "ofnnthe\n",
      "estimatorsnnand\n",
      "nndefine\n",
      "andnnen\n",
      "inn\n",
      "enn\n",
      "inn\n",
      "nn\n",
      "nnxe\n",
      "nn\n",
      "nn\n",
      "nnenn\n",
      "nn\n",
      "ninn\n",
      "enn\n",
      "nnxe\n",
      "nn\n",
      "nn\n",
      "nnxe\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nnletndxe\n",
      "nnmators\n",
      "npnnand\n",
      "nntheorem\n",
      "thennxe\n",
      "ann\n",
      "nnpnn\n",
      "nnxe\n",
      "nn\n",
      "nn\n",
      "nnxe\n",
      "nn\n",
      "nnnnvariable\n",
      "nnthe\n",
      "nknnand\n",
      "definennxcexa\n",
      "enn\n",
      "nne\n",
      "tinn\n",
      "nntheorem\n",
      "thennxe\n",
      "ann\n",
      "nnknn\n",
      "nnxe\n",
      "nn\n",
      "nn\n",
      "nnxe\n",
      "nn\n",
      "nnremark\n",
      "truennweights\n",
      "tonnn\n",
      "ofnnan\n",
      "nndiscuss\n",
      "resultsnnfrom\n",
      "nnwhile\n",
      "nnthe\n",
      "nnremark\n",
      "andnnb\n",
      "nn\n",
      "enn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nnen\n",
      "nnxe\n",
      "tinn\n",
      "nn\n",
      "nnxe\n",
      "nknn\n",
      "npnn\n",
      "ifnna\n",
      "nn\n",
      "nn\n",
      "resultsnnfound\n",
      "meannnmodels\n",
      "regressionnnmodels\n",
      "nnremark\n",
      "nnand\n",
      "nnthey\n",
      "nncontrast\n",
      "outlinesnnwhy\n",
      "locationnnscale\n",
      "modelnnyi\n",
      "nndxe\n",
      "nngjc\n",
      "nnxefxa\n",
      "nngjs\n",
      "nnxefxa\n",
      "nnthat\n",
      "fnnxe\n",
      "nnxcexb\n",
      "fnnxe\n",
      "thenndistribution\n",
      "ofnn\n",
      "isnnfy\n",
      "nnxe\n",
      "nnthe\n",
      "thatnnare\n",
      "thennsame\n",
      "termsnnand\n",
      "thennlocation\n",
      "holdnnunder\n",
      "nn\n",
      "asymptoticsnnunder\n",
      "dimensionalnncase\n",
      "nnto\n",
      "withnnmissing\n",
      "modelnnli\n",
      "arennnnvariable\n",
      "nnnot\n",
      "nnbut\n",
      "nnthe\n",
      "nnlem\n",
      "benna\n",
      "wouldnnhave\n",
      "nnvariates\n",
      "andnnhave\n",
      "nnthe\n",
      "nnmultaneous\n",
      "fornnthe\n",
      "dimensionalnnsetting\n",
      "nncondition\n",
      "nnand\n",
      "onennc\n",
      "nnxe\n",
      "nann\n",
      "nnxe\n",
      "nn\n",
      "nnnn\n",
      "nncondition\n",
      "nnc\n",
      "nnfor\n",
      "nnthe\n",
      "fornnthe\n",
      "nntheorem\n",
      "nnxe\n",
      "nn\n",
      "nn\n",
      "opnn\n",
      "nn\n",
      "nn\n",
      "nnthe\n",
      "fornnthe\n",
      "nntheorem\n",
      "ratennof\n",
      "nn\n",
      "maxnnin\n",
      "nnnnvariable\n",
      "nnopn\n",
      "lnn\n",
      "nn\n",
      "nn\n",
      "opnn\n",
      "nn\n",
      "nn\n",
      "nnremark\n",
      "nnthe\n",
      "fixednndimensional\n",
      "nnxe\n",
      "nn\n",
      "nnwise\n",
      "mentionednnin\n",
      "fixednndimensions\n",
      "nnric\n",
      "muchnnmore\n",
      "priorinnknowledge\n",
      "ofnnthe\n",
      "withnnthe\n",
      "penaltynncould\n",
      "wouldnnrequire\n",
      "nnin\n",
      "methodnnwas\n",
      "alleviatennbias\n",
      "nn\n",
      "selectionnn\n",
      "functionnnin\n",
      "nnthe\n",
      "rigorouslynnestimate\n",
      "missingnndata\n",
      "nnxe\n",
      "nnpxe\n",
      "nnpxcexbb\n",
      "nnthe\n",
      "nnpenalized\n",
      "modelnnnnvariable\n",
      "nnselection\n",
      "cannnbe\n",
      "whennnconsidering\n",
      "nnular\n",
      "selectionnconsistency\n",
      "nnlationship\n",
      "fannnand\n",
      "nntion\n",
      "proposednnmcp\n",
      "nnthe\n",
      "formnnpxcexbb\n",
      "nn\n",
      "nn\n",
      "nnaxe\n",
      "nn\n",
      "nn\n",
      "nnwhile\n",
      "nnpxcexbb\n",
      "nn\n",
      "nn\n",
      "axcexbbnn\n",
      "axcexbbnn\n",
      "nn\n",
      "nnfigure\n",
      "fornnxcexbb\n",
      "isnnthey\n",
      "zeronnas\n",
      "andnna\n",
      "penaltynnfunctions\n",
      "modelnnand\n",
      "nnthe\n",
      "nn\n",
      "qnnn\n",
      "nnxcexb\n",
      "nn\n",
      "minn\n",
      "nn\n",
      "nn\n",
      "inn\n",
      "qnn\n",
      "nnlet\n",
      "nknnbe\n",
      "nnthe\n",
      "donnnot\n",
      "thennnnvariable\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nnxcexb\n",
      "nnp\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nnxcexb\n",
      "nnp\n",
      "nnxcexb\n",
      "nnlasso\n",
      "mcpnnfigure\n",
      "nnestimators\n",
      "thereforennthe\n",
      "thennhigh\n",
      "quicklynna\n",
      "nncondition\n",
      "andnnc\n",
      "nn\n",
      "minnn\n",
      "qnn\n",
      "nnthe\n",
      "arennequivalent\n",
      "penaltynnfunction\n",
      "nntheorem\n",
      "parametricnnweights\n",
      "letnnxe\n",
      "nn\n",
      "nnbe\n",
      "nnxe\n",
      "nn\n",
      "onn\n",
      "nnxe\n",
      "nn\n",
      "nnnxe\n",
      "nno\n",
      "thennprn\n",
      "nn\n",
      "nnnnvariable\n",
      "nntheorem\n",
      "nnric\n",
      "letnnxe\n",
      "nn\n",
      "nnbe\n",
      "thennkernel\n",
      "nn\n",
      "maxnnin\n",
      "opnn\n",
      "lnn\n",
      "nn\n",
      "ifnnxcexbb\n",
      "nnxe\n",
      "nn\n",
      "nnnxe\n",
      "lnn\n",
      "nnand\n",
      "thennnprn\n",
      "nn\n",
      "nntheorem\n",
      "lnn\n",
      "nnbecause\n",
      "letnnh\n",
      "nnxe\n",
      "nn\n",
      "nn\n",
      "andnnc\n",
      "nnas\n",
      "thennconditions\n",
      "nnxcexb\n",
      "ofnnthe\n",
      "nnthe\n",
      "statednnin\n",
      "nn\n",
      "lnn\n",
      "nonparametricnnweights\n",
      "cannnbe\n",
      "presentnnsufficient\n",
      "nnthe\n",
      "asymptoticallynnthe\n",
      "annn\n",
      "andnnin\n",
      "nnnnvariable\n",
      "nn\n",
      "studiesnn\n",
      "estimatornnthe\n",
      "linearnnapproximation\n",
      "solvingnn\n",
      "nn\n",
      "minn\n",
      "nn\n",
      "nnxe\n",
      "nnnxe\n",
      "nnpnxe\n",
      "nnpxe\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nnwith\n",
      "nn\n",
      "nn\n",
      "sufficientlynnclose\n",
      "nn\n",
      "lassonnpenalty\n",
      "isnnadapted\n",
      "thatnn\n",
      "nn\n",
      "resultsnnmonte\n",
      "sizennperformance\n",
      "nnerated\n",
      "ann\n",
      "ofnnxe\n",
      "thennlinear\n",
      "nnmal\n",
      "generatingnnmechanism\n",
      "isnnyi\n",
      "nnin\n",
      "whennnxefxbfxbdi\n",
      "considerednn\n",
      "nnnnvariable\n",
      "nn\n",
      "nnin\n",
      "therennare\n",
      "usednnto\n",
      "influencennthe\n",
      "thennvariables\n",
      "binomialnnglm\n",
      "afternna\n",
      "finalnnmodel\n",
      "variablennis\n",
      "nnin\n",
      "estimatednnweights\n",
      "weightnnbeing\n",
      "andnnrelates\n",
      "nnability\n",
      "thennbenefits\n",
      "arennreported\n",
      "estimatornnthat\n",
      "weightsnn\n",
      "nnsupplementary\n",
      "lassonnand\n",
      "modelnnand\n",
      "nnthree\n",
      "nnbles\n",
      "mnnindexes\n",
      "nnxe\n",
      "nnxe\n",
      "nnxe\n",
      "nnxe\n",
      "nnxe\n",
      "pnnj\n",
      "nnxe\n",
      "nnnnvariable\n",
      "nnxe\n",
      "pnnj\n",
      "nnxe\n",
      "nn\n",
      "nmnnj\n",
      "nn\n",
      "nnxe\n",
      "nnm\n",
      "nnxe\n",
      "nnni\n",
      "nnzero\n",
      "nnlinear\n",
      "ofnninternal\n",
      "andnnxcexbd\n",
      "totalnnnumber\n",
      "nnand\n",
      "nnnand\n",
      "minimizingnnqbicw\n",
      "lnnn\n",
      "nnxe\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nn\n",
      "nnfor\n",
      "cscadnnfullxe\n",
      "nnproposed\n",
      "quantilennregression\n",
      "bynnfan\n",
      "nnsimulations\n",
      "nntimates\n",
      "regrnnfrom\n",
      "usingnna\n",
      "ofnnsepanski\n",
      "tnnxe\n",
      "nntimated\n",
      "nncomes\n",
      "resultsnnfor\n",
      "nnreport\n",
      "thatnnthere\n",
      "reducennthe\n",
      "nntable\n",
      "nnin\n",
      "weightsnnnnvariable\n",
      "nnand\n",
      "modelnnholds\n",
      "simulationsnncorresponding\n",
      "basednnestimates\n",
      "nnsented\n",
      "parametricnnmodel\n",
      "suggestingnnthat\n",
      "parametricnnmethod\n",
      "nntables\n",
      "nnparametric\n",
      "modelnnis\n",
      "nnsional\n",
      "estimatednncoefficients\n",
      "nncuracy\n",
      "nnmethod\n",
      "aadennscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nntable\n",
      "nn\n",
      "analysisnnthe\n",
      "annrehabilitation\n",
      "ofnnnnvariable\n",
      "nnmethod\n",
      "aadennscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nntable\n",
      "nnhosmer\n",
      "nnthe\n",
      "andnnrandomized\n",
      "nnage\n",
      "andnnlength\n",
      "nnbeck\n",
      "usennand\n",
      "lengthnnof\n",
      "regressionnnwith\n",
      "thennpredictors\n",
      "whichnnshows\n",
      "nna\n",
      "usednnto\n",
      "nnear\n",
      "quantilennregression\n",
      "asnnwqbic\n",
      "lnnn\n",
      "nnxe\n",
      "nn\n",
      "nnln\n",
      "nn\n",
      "nn\n",
      "nnwhere\n",
      "samplennnnvariable\n",
      "nnmethod\n",
      "aadennscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nntable\n",
      "nnand\n",
      "ifnna\n",
      "arennconsidered\n",
      "onlynnpredictor\n",
      "nonlinearnnmodels\n",
      "modelnnis\n",
      "nnif\n",
      "asnna\n",
      "doesnnnot\n",
      "bennconditionally\n",
      "usingnnthe\n",
      "minimizesnn\n",
      "andnn\n",
      "nnnone\n",
      "thenn\n",
      "nnthus\n",
      "andnnnot\n",
      "isnnthe\n",
      "nnto\n",
      "randomlynnnnvariable\n",
      "nnmethod\n",
      "aadennscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nnscad\n",
      "nntable\n",
      "nnestimate\n",
      "nnrandomized\n",
      "nntreatment\n",
      "nnlength\n",
      "nntime\n",
      "nntable\n",
      "missingnessnnpartitioned\n",
      "setnnwith\n",
      "thenntraining\n",
      "wqbicnnapproach\n",
      "scadnnpenalized\n",
      "nning\n",
      "nonparametricnnweights\n",
      "estimatenna\n",
      "repeatednn\n",
      "nnpercentage\n",
      "averagennprediction\n",
      "deviationnnof\n",
      "capturennnnvariable\n",
      "nnrates\n",
      "arennclose\n",
      "intervalnnlengths\n",
      "nntervals\n",
      "afternnthe\n",
      "nndiction\n",
      "dramaticallynnoutperforming\n",
      "nnaround\n",
      "lastnnpoint\n",
      "variablennwhen\n",
      "anndrastically\n",
      "ifnnignoring\n",
      "nnmethod\n",
      "lengthnnnaive\n",
      "nnparametric\n",
      "nnkernel\n",
      "nntable\n",
      "resultsnn\n",
      "discussionnnthis\n",
      "nnsion\n",
      "nonlinearnnrelationships\n",
      "missingnncovariates\n",
      "estimationnnand\n",
      "nnthe\n",
      "thisnnensures\n",
      "variablesnnhave\n",
      "drivennnmethods\n",
      "usednnwill\n",
      "simulationsnnand\n",
      "nnserved\n",
      "thennnnvariable\n",
      "nnaccuracy\n",
      "thennpresence\n",
      "nna\n",
      "nnlem\n",
      "fornnhandling\n",
      "proposednnby\n",
      "andnnruppert\n",
      "bynnincluding\n",
      "originalnnvariable\n",
      "nonlinearnnterms\n",
      "fornnthe\n",
      "areannof\n",
      "nn\n",
      "appendixnnproofsnndefinition\n",
      "functionsnnthroughout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doesnnnot\n",
      "nnxe\n",
      "spectralnnnorm\n",
      "nnproofs\n",
      "nnmates\n",
      "parametricnnproofs\n",
      "thennkernel\n",
      "anncommon\n",
      "thenntaylor\n",
      "proofsnntypically\n",
      "withnnn\n",
      "nnbecause\n",
      "ofnnthis\n",
      "positivennconstant\n",
      "nnnnvariable\n",
      "nnfor\n",
      "nnlar\n",
      "sectionn\n",
      "definingnnwj\n",
      "anknnn\n",
      "nne\n",
      "nnb\n",
      "nn\n",
      "nn\n",
      "ofnnbasis\n",
      "nn\n",
      "nnwhere\n",
      "nnxcexb\n",
      "nn\n",
      "nn\n",
      "minn\n",
      "nnnxe\n",
      "nn\n",
      "nnthen\n",
      "nn\n",
      "nknn\n",
      "nn\n",
      "thennnonlinear\n",
      "thennunknown\n",
      "nknn\n",
      "nnwhile\n",
      "nn\n",
      "nndn\n",
      "nn\n",
      "inn\n",
      "nnw\n",
      "nnp\n",
      "nnxxe\n",
      "nn\n",
      "nnw\n",
      "nnxcexb\n",
      "ann\n",
      "nnxcexb\n",
      "nnxe\n",
      "rnnjn\n",
      "nn\n",
      "nnuni\n",
      "nnqi\n",
      "nn\n",
      "uninn\n",
      "nnes\n",
      "nnnnvariable\n",
      "nnnotice\n",
      "thatnnnxe\n",
      "nn\n",
      "nnnxe\n",
      "nndefining\n",
      "minn\n",
      "nnxe\n",
      "nni\n",
      "thennnxcexb\n",
      "annn\n",
      "nknn\n",
      "nnand\n",
      "wdnn\n",
      "nn\n",
      "wnn\n",
      "nknn\n",
      "thenngrowing\n",
      "nnlemma\n",
      "whichnnprovides\n",
      "nnlemma\n",
      "nnas\n",
      "supnntnnxe\n",
      "ennsupntnnxe\n",
      "nhnnxe\n",
      "nn\n",
      "nknn\n",
      "tnhnn\n",
      "nknn\n",
      "tnhnn\n",
      "enn\n",
      "nknn\n",
      "tnhnn\n",
      "nknn\n",
      "tnhnn\n",
      "nn\n",
      "opnn\n",
      "nnnhnn\n",
      "nnif\n",
      "functionnnsupntnnxe\n",
      "nhnnxe\n",
      "nnknn\n",
      "tnhnn\n",
      "nn\n",
      "nne\n",
      "nnxe\n",
      "nn\n",
      "nnnhnn\n",
      "nnproof\n",
      "nnand\n",
      "nnthe\n",
      "nnlemma\n",
      "nn\n",
      "sufficientlynnlarge\n",
      "nnnnvariable\n",
      "nn\n",
      "sufficientlynnlarge\n",
      "nn\n",
      "opnn\n",
      "nnn\n",
      "nnproof\n",
      "thatnncondition\n",
      "thennuniform\n",
      "estimatednnweights\n",
      "nnproofs\n",
      "supplementarynnmaterial\n",
      "nnlemma\n",
      "nnop\n",
      "nnxe\n",
      "nxnnxe\n",
      "nntion\n",
      "nnlemma\n",
      "nnprnn\n",
      "lnndxe\n",
      "nnnnxe\n",
      "inn\n",
      "nn\n",
      "nnproof\n",
      "nnproof\n",
      "fromnnschumaker\n",
      "thisnnwith\n",
      "thennnnxe\n",
      "nnxe\n",
      "nnfi\n",
      "nnxe\n",
      "nnfi\n",
      "nn\n",
      "nnxe\n",
      "nnxe\n",
      "nn\n",
      "opnn\n",
      "nnproof\n",
      "uppernnbound\n",
      "nnnnvariable\n",
      "nnfor\n",
      "nknn\n",
      "thennrate\n",
      "casennof\n",
      "nnproof\n",
      "nnproof\n",
      "ann\n",
      "nnkxe\n",
      "nnnxe\n",
      "nni\n",
      "inn\n",
      "nnni\n",
      "nnusing\n",
      "supplementarynnmaterial\n",
      "secondnnxe\n",
      "ann\n",
      "nnw\n",
      "nnxe\n",
      "nn\n",
      "nn\n",
      "nnxcexb\n",
      "nnri\n",
      "nne\n",
      "nn\n",
      "nnthe\n",
      "thennvariance\n",
      "isnnvarnn\n",
      "nnxcexb\n",
      "nn\n",
      "enn\n",
      "nnxcexb\n",
      "nn\n",
      "nn\n",
      "enn\n",
      "nn\n",
      "nn\n",
      "nnthe\n",
      "isnnvarnn\n",
      "nne\n",
      "nn\n",
      "enn\n",
      "nn\n",
      "nn\n",
      "enn\n",
      "nne\n",
      "nnfor\n",
      "andnnthe\n",
      "getnncovnn\n",
      "nnxcexb\n",
      "nne\n",
      "nn\n",
      "enn\n",
      "nn\n",
      "enn\n",
      "nne\n",
      "nnnnvariable\n",
      "nnproof\n",
      "nnproof\n",
      "modelnnsn\n",
      "nndefinenn\n",
      "nn\n",
      "nnthe\n",
      "componentsnnfor\n",
      "nnof\n",
      "bynnsj\n",
      "nn\n",
      "nnxe\n",
      "nnsj\n",
      "nn\n",
      "nnxe\n",
      "nnwhere\n",
      "fromnn\n",
      "isnnsufficient\n",
      "onennsjnn\n",
      "nn\n",
      "nnnnvariable\n",
      "holdsnnfrom\n",
      "innntheorem\n",
      "nnlet\n",
      "pnnnsjnn\n",
      "nn\n",
      "nn\n",
      "inxijnn\n",
      "nn\n",
      "nnnnxe\n",
      "annxe\n",
      "nnwhere\n",
      "nnxe\n",
      "annxe\n",
      "nnopn\n",
      "nnnxe\n",
      "nncn\n",
      "lnn\n",
      "nn\n",
      "ofnntheorem\n",
      "shownnprnn\n",
      "nmaxnnqn\n",
      "pnnnxe\n",
      "nnnxe\n",
      "inxijnn\n",
      "nn\n",
      "mnnn\n",
      "nnproof\n",
      "modifiednnto\n",
      "shownnmaxnqn\n",
      "pnnnxe\n",
      "nnnxe\n",
      "nnxijnn\n",
      "nn\n",
      "conditionn\n",
      "maxnnin\n",
      "undernnthe\n",
      "thennnmaxnqn\n",
      "pnnnxe\n",
      "nnnxe\n",
      "nnrinn\n",
      "nn\n",
      "nxijnn\n",
      "nn\n",
      "cnn\n",
      "nn\n",
      "nmaxninnxe\n",
      "nnxe\n",
      "lnn\n",
      "nn\n",
      "nnwith\n",
      "nn\n",
      "nno\n",
      "nnnnvariable\n",
      "nnacknowledgments\n",
      "fornntheir\n",
      "significantlynnimprove\n",
      "nnreferencesnnchen\n",
      "analysisnnwith\n",
      "nncheng\n",
      "nnsion\n",
      "nnde\n",
      "withnnhigh\n",
      "nnfan\n",
      "likelihoodnnand\n",
      "nnfan\n",
      "linearnnmodels\n",
      "ann\n",
      "nnhe\n",
      "linearnnmodel\n",
      "nnhe\n",
      "modelnnfor\n",
      "biometrikann\n",
      "nnhorowitz\n",
      "nntile\n",
      "nnhosmer\n",
      "regressionnnmodeling\n",
      "nnhuang\n",
      "nnsinica\n",
      "nnklemela\n",
      "nnavailable\n",
      "nnnnvariable\n",
      "nnkoenker\n",
      "nnkoenker\n",
      "nn\n",
      "nnlavergne\n",
      "nnworking\n",
      "nnlian\n",
      "intonnnonparametric\n",
      "linearnnadditive\n",
      "nnliang\n",
      "partiallynnlinear\n",
      "nn\n",
      "nnlipsitz\n",
      "quantilennregression\n",
      "nncell\n",
      "nnr\n",
      "nnliu\n",
      "nnates\n",
      "nnliu\n",
      "fornnsemiparametric\n",
      "nnnadaraya\n",
      "nn\n",
      "nnrobins\n",
      "coefficientsnnwhen\n",
      "nn\n",
      "nnschwarz\n",
      "ann\n",
      "nnschumaker\n",
      "nnsepanski\n",
      "nnrection\n",
      "nnnnvariable\n",
      "nnsherwood\n",
      "rnnpackage\n",
      "nnsherwood\n",
      "nnsion\n",
      "ann\n",
      "nnsherwood\n",
      "fornnanalyzing\n",
      "nn\n",
      "nnstone\n",
      "ann\n",
      "nnstat\n",
      "nntao\n",
      "nntheory\n",
      "nntibshirani\n",
      "nnstat\n",
      "nntripathi\n",
      "nneconomics\n",
      "nnwang\n",
      "semiparametricnnestimation\n",
      "nnstatist\n",
      "nnwang\n",
      "nngression\n",
      "ann\n",
      "nn\n",
      "nnwang\n",
      "linearnnvarying\n",
      "ann\n",
      "nnwang\n",
      "nngeneity\n",
      "nnwatson\n",
      "nnwei\n",
      "nngression\n",
      "nnnnvariable\n",
      "nnwei\n",
      "atnnrandom\n",
      "nnwu\n",
      "sinicann\n",
      "nnyi\n",
      "withnndropouts\n",
      "nnzhang\n",
      "concavennpenalty\n",
      "ann\n",
      "nnzhang\n",
      "automaticnnstructure\n",
      "nn\n",
      "nnzhao\n",
      "nn\n",
      "nnzou\n",
      "penalizednnlikelihood\n",
      "ann\n",
      "nnjohns\n",
      "universitynne\n",
      "edunnnt\n",
      "introductionnt\n",
      "regressionnt\n",
      "asymptoticsnnt\n",
      "selectionnt\n",
      "functionnnt\n",
      "resultsnnt\n",
      "discussionnt\n",
      "appendixnn\n",
      "After correction: 6256\n"
     ]
    }
   ],
   "source": [
    "# all in one\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import enchant\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "d = enchant.Dict(\"en_US\")\n",
    "words_cleaned = []\n",
    "\n",
    "#removing stopwords\n",
    "words = [w for w in paper_token if not w in stop_words]\n",
    "print('After stopwords removal:',len(words))\n",
    "#cleaning from nonsense 'x'-expressions (equations)\n",
    "words_c = [w for w in words if not w.startswith('x') and not w.endswith('x') and 'nnnnn' not in w]\n",
    "print('After another removal:',len(words_c))\n",
    "\n",
    "#correction of words broken into two parts\n",
    "for w in range(len(words_c)):\n",
    "    if 'nn' in words_c[w]:\n",
    "        print(words_c[w])\n",
    "    \n",
    "    \n",
    "    if words_c[w].startswith('nn'):\n",
    "        clean=words_c[w].replace('nn','')\n",
    "        clean1=words_c[w-1]+clean\n",
    "        if d.check(clean1) == True:\n",
    "            words_cleaned.append(clean1)\n",
    "        if len(clean) > 0 and d.check(clean) == True:  \n",
    "            words_cleaned.append(clean)\n",
    "    elif (len(words_c[w]) > 1) and (d.check(words_c[w]) == True):\n",
    "        words_cleaned.append(words_c[w])\n",
    "    elif len(words_c[w]) > 1:\n",
    "        words_cleaned.append(words_c[w])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "print('After correction:',len(words_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRONG!!!!!!!!!!!!!!!!!!!! nnmensional 90\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnlations 122\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 147\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnvides 155\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 202\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 204\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 206\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 208\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 211\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! jnnunn 213\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 214\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nning 238\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! zhounn 319\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 399\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nna 453\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnwang 492\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnproach 500\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnlipsitz 524\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nndinal 532\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nning 628\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntive 737\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nning 776\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnates 798\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 937\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnmance 954\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 993\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsion 1047\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1118\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsubintervals 1163\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntions 1180\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1221\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1231\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1235\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntions 1251\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 1298\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1300\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1326\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nna 1450\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1468\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! argminn 1469\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1583\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1584\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1588\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 1636\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnj 1637\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnj 1641\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1645\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! npnn 1698\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1700\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnpn 1718\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnliang 1719\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 1746\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1748\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1755\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1767\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1799\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 1863\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! npnn 1897\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnficient 1914\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsity 1930\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnstant 1957\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2010\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnh 2062\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! inn 2162\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 2164\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! inn 2166\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2167\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2169\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2170\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2172\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnenn 2174\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2175\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ninn 2179\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 2181\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2183\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2184\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2186\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2188\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2189\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2191\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2192\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnletndxe 2194\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnmators 2201\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 2209\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnpnn 2210\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2214\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2216\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2218\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2224\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2226\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 2244\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nne 2248\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 2259\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnknn 2260\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2264\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2266\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2268\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2274\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2276\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2358\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 2360\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2362\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2365\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2368\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2374\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2380\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2387\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 2395\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! npnn 2399\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2405\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2408\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nndxe 2481\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nngjc 2483\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxefxa 2485\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nngjs 2488\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxefxa 2490\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 2516\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2549\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2618\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnlem 2700\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnmultaneous 2744\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2783\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nann 2784\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2788\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2790\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnc 2799\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2817\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2818\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2821\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2825\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2827\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2856\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnopn 2871\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! lnn 2873\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2875\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2878\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2882\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2884\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 2909\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 2910\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnric 2931\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3001\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 3027\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnpxe 3032\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnpxcexbb 3034\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnular 3080\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnlationship 3094\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntion 3109\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3126\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3128\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnaxe 3130\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3133\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3135\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnpxcexbb 3142\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3144\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3145\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! axcexbbnn 3146\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! axcexbbnn 3149\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3150\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3206\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 3213\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3216\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3219\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3221\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! inn 3223\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! qnn 3227\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3262\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3263\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3265\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3267\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 3269\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnp 3270\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3271\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3272\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3273\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3275\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3277\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3279\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3281\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3283\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 3285\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnp 3286\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 3288\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3338\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! qnn 3342\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3383\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 3393\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3394\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! onn 3396\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 3397\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3399\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nno 3407\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3415\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnric 3440\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3450\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3468\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! lnn 3476\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3478\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 3481\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3483\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! lnn 3490\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3502\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! lnn 3509\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 3519\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3521\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3527\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 3552\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3579\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! lnn 3580\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3636\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3663\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3666\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 3667\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnpnxe 3673\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnpxe 3675\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3679\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3682\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3684\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3690\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3692\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3704\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3746\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnerated 3763\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 3770\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnmal 3786\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 3840\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnbles 4001\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4015\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4022\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4031\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4040\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4048\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4052\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4064\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4068\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4070\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4075\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4076\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnm 4083\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4084\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4157\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4170\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4171\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4175\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4176\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntimates 4217\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntimated 4248\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsented 4338\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsional 4381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4448\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnhosmer 4522\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nna 4615\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 4643\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4646\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4650\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 4651\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nning 4955\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntervals 5030\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsion 5114\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nna 5215\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnlem 5223\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5283\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 5306\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! sectionn 5412\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nne 5420\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnb 5426\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5428\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5440\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5453\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 5469\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5471\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5473\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5482\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5485\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 5486\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5491\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 5509\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5518\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nndn 5520\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5524\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! inn 5531\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnw 5535\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnp 5543\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxxe 5552\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5553\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnw 5558\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 5563\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 5565\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 5567\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 5573\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5583\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5594\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5620\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 5628\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nni 5629\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 5637\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5643\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! wnn 5646\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 5648\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5718\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 5719\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! tnhnn 5721\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 5722\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! tnhnn 5724\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 5727\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 5728\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! tnhnn 5730\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 5731\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! tnhnn 5733\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5736\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnknn 5755\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! tnhnn 5757\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5761\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nne 5763\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 5767\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5770\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5798\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5819\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5849\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 5896\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntion 5901\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! inn 5911\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5916\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 5943\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnfi 5945\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 5951\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnfi 5953\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5958\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 5960\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 5964\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 5967\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nknn 5994\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 6018\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnkxe 6019\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nni 6022\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! inn 6026\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 6039\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnw 6040\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 6043\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6047\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6050\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 6053\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnri 6056\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nne 6057\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6059\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! isnnvarnn 6078\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 6079\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6080\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 6082\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 6083\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6087\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6088\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 6090\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6096\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6097\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! isnnvarnn 6103\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nne 6105\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6108\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 6109\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6116\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6117\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 6118\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nne 6120\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! getnncovnn 6138\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxcexb 6139\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nne 6141\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6144\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 6145\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6151\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! enn 6153\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nne 6155\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6196\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6235\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 6244\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsj 6249\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6260\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 6272\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! onennsjnn 6310\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6312\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6368\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6370\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! inxijnn 6372\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6378\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 6403\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnopn 6407\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nncn 6418\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! lnn 6420\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6422\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! shownnprnn 6430\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! inxijnn 6436\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6441\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxijnn 6455\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6460\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! conditionn 6469\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnrinn 6485\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6488\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nxijnn 6489\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6494\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! cnn 6496\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6499\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnxe 6503\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! lnn 6506\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6508\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6520\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nno 6522\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnreferencesnnchen 6550\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nncheng 6563\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsion 6571\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 6616\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! biometrikann 6644\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnhorowitz 6645\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnhosmer 6659\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnhuang 6674\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsinica 6682\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnklemela 6683\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnkoenker 6710\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnkoenker 6717\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6724\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnlavergne 6725\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnlian 6737\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnliang 6754\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6770\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnlipsitz 6771\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnr 6794\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnliu 6799\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnates 6806\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnliu 6811\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6833\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6849\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnschwarz 6850\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 6855\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnschumaker 6857\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsepanski 6866\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnrection 6874\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsherwood 6889\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsherwood 6908\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsion 6917\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 6921\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnsherwood 6923\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 6940\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 6947\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntao 6949\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntibshirani 6962\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nntripathi 6975\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnwang 6983\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnwang 7000\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nngression 7010\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 7016\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 7018\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnwang 7019\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 7031\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnwang 7033\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nngeneity 7042\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnwatson 7050\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnwei 7057\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nngression 7063\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnwei 7074\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnwu 7083\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! sinicann 7090\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnyi 7091\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnzhang 7101\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 7108\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnzhang 7110\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 7126\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnzhao 7127\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nn 7136\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! nnzou 7137\n",
      "WRONG!!!!!!!!!!!!!!!!!!!! ann 7148\n",
      "6328\n",
      "1150\n"
     ]
    }
   ],
   "source": [
    "ns=[]\n",
    "n=0\n",
    "words_cleaned=[]\n",
    "wrong=[]\n",
    "\n",
    "for w in range(len(words_c)):\n",
    "    n+=1\n",
    "    if 'nnn' in words_c[w]:\n",
    "        ns.append(words_c[w])\n",
    "        #print('THREE N-S',words_c[w])\n",
    "        j=words_c[w][words_c[w].find('nn')+2:len(i)]\n",
    "        k=j[1:]\n",
    "        l=words_c[w][:words_c[w].find('nn')+1]\n",
    "        if len(j) > 1 and d.check(j) == True:\n",
    "            words_cleaned.append(j)\n",
    "            #print('begins with n',j,n)\n",
    "        if len(k) > 2 and d.check(k) == True:\n",
    "            words_cleaned.append(k)\n",
    "            #print('CLEANED M',k,n)\n",
    "        if len(l) > 1 and d.check(l) == True:\n",
    "            words_cleaned.append(l)\n",
    "            #print('first part',l,n)\n",
    "        \n",
    "\n",
    "    elif 'nn' in words_c[w]:\n",
    "        if words_c[w].startswith('nn'):\n",
    "            clean=words_c[w].replace('nn','')\n",
    "            clean1=words_c[w-1]+clean\n",
    "            if d.check(clean1) == True:\n",
    "                words_cleaned.append(clean1)\n",
    "                #print('CLEANED S',clean1,n)\n",
    "            if len(clean) > 1 and d.check(clean) == True:\n",
    "                words_cleaned.append(clean)\n",
    "                #print('CLEANED S',clean,n)\n",
    "            else:\n",
    "                wrong.append(words_c[w])\n",
    "                print('WRONG!!!!!!!!!!!!!!!!!!!!',words_c[w],n)\n",
    "        elif words_c[w].endswith('nn'):\n",
    "            clean=words_c[w].replace('nn','')\n",
    "            if n < len(words_c)-1:\n",
    "                clean1=clean+words_c[w+1]\n",
    "            else:\n",
    "                clean1=clean+words_c[w]\n",
    "\n",
    "            if d.check(clean1) == True:\n",
    "                words_cleaned.append(clean1)\n",
    "                #print('CLEANED E',clean1,n)\n",
    "            if len(clean) > 1 and d.check(clean) == True:\n",
    "                words_cleaned.append(clean)\n",
    "                #print('CLEANED E',clean,n)\n",
    "            else:\n",
    "                wrong.append(words_c[w])\n",
    "                print('WRONG!!!!!!!!!!!!!!!!!!!!',words_c[w],n)\n",
    "        else:\n",
    "            j=words_c[w][words_c[w].find('nn')+1:len(words_c[w])]\n",
    "            k=j[1:]\n",
    "            l=words_c[w][:words_c[w].find('nn')]\n",
    "            if len(j) > 1 and d.check(j) == True:\n",
    "                words_cleaned.append(j)\n",
    "                #print('begins with n',j)\n",
    "            if len(k) > 2 and d.check(k) == True:\n",
    "                words_cleaned.append(k)\n",
    "                #print('CLEANED M',k)\n",
    "            if len(l) > 1 and d.check(l) == True:\n",
    "                words_cleaned.append(l)\n",
    "                #print('first part',l)\n",
    "            \n",
    "    elif (len(words_c[w]) > 2) and (d.check(words_c[w]) == True):\n",
    "        words_cleaned.append(words_c[w])\n",
    "    elif len(words_c[w]) > 2:\n",
    "        words_cleaned.append(words_c[w])\n",
    "    else:    \n",
    "        wrong.append(words_c[w])\n",
    "        #print(n)\n",
    "        continue\n",
    "\n",
    "     \n",
    "        \n",
    "print(len(words_cleaned))\n",
    "print(len(wrong))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANED M ben\n",
      "CLEANED M johns\n",
      "CLEANED M abstract\n",
      "first part university\n",
      "CLEANED M the\n",
      "first part at\n",
      "CLEANED M relaxed\n",
      "first part are\n",
      "CLEANED M objective\n",
      "first part weighted\n",
      "CLEANED M potential\n",
      "first part the\n",
      "CLEANED M are\n",
      "CLEANED M penalties\n",
      "CLEANED M presence\n",
      "first part the\n",
      "CLEANED M where\n",
      "first part cases\n",
      "CLEANED M function\n",
      "first part penalty\n",
      "CLEANED M leaving\n",
      "first part patients\n",
      "CLEANED M probability\n",
      "first part inverse\n",
      "CLEANED M assumptions\n",
      "first part distributional\n",
      "CLEANED M paper\n",
      "first part this\n",
      "first part missing\n",
      "CLEANED M caused\n",
      "first part bias\n",
      "first part or\n",
      "CLEANED M function\n",
      "first part objective\n",
      "first part presence\n",
      "CLEANED M and\n",
      "first part model\n",
      "CLEANED M allows\n",
      "first part still\n",
      "CLEANED M reasonable\n",
      "first part is\n",
      "begins with n nit\n",
      "first part curse\n",
      "CLEANED M function\n",
      "first part nonlinear\n",
      "CLEANED M rest\n",
      "first part the\n",
      "first part case\n",
      "CLEANED M case\n",
      "first part increasing\n",
      "begins with n net\n",
      "first part robins\n",
      "CLEANED M complete\n",
      "first part with\n",
      "CLEANED M are\n",
      "first part weights\n",
      "first part for\n",
      "CLEANED M observed\n",
      "first part same\n",
      "CLEANED M missing\n",
      "first part with\n",
      "CLEANED M with\n",
      "first part model\n",
      "CLEANED M linear\n",
      "first part partial\n",
      "first part the\n",
      "CLEANED M with\n",
      "first part procedure\n",
      "first part missing\n",
      "CLEANED M multiple\n",
      "first part caused\n",
      "CLEANED M bias\n",
      "first part handle\n",
      "CLEANED M location\n",
      "CLEANED M location\n",
      "first part the\n",
      "CLEANED M using\n",
      "first part efficiency\n",
      "CLEANED M equation\n",
      "first part estimating\n",
      "first part methods\n",
      "CLEANED M assumption\n",
      "first part linearity\n",
      "first part using\n",
      "first part model\n",
      "CLEANED M settings\n",
      "first part of\n",
      "CLEANED M estimated\n",
      "first part are\n",
      "CLEANED M fully\n",
      "first part proposed\n",
      "CLEANED M partial\n",
      "first part the\n",
      "CLEANED M missing\n",
      "first part of\n",
      "first part reasonable\n",
      "first part case\n",
      "CLEANED M these\n",
      "first part address\n",
      "CLEANED M partial\n",
      "first part the\n",
      "begins with n nates\n",
      "first part potential\n",
      "CLEANED M objective\n",
      "first part weighted\n",
      "CLEANED M terms\n",
      "first part linear\n",
      "CLEANED M proposed\n",
      "first part is\n",
      "CLEANED M function\n",
      "first part objective\n",
      "CLEANED M has\n",
      "first part estimator\n",
      "CLEANED M objective\n",
      "first part as\n",
      "CLEANED M scad\n",
      "first part the\n",
      "CLEANED M mean\n",
      "first part linear\n",
      "CLEANED M the\n",
      "first part with\n",
      "first part for\n",
      "CLEANED M linear\n",
      "first part partial\n",
      "first part linear\n",
      "CLEANED M linear\n",
      "first part of\n",
      "CLEANED M sober\n",
      "first part time\n",
      "first part includes\n",
      "begins with n nor\n",
      "first part linear\n",
      "CLEANED M directions\n",
      "first part of\n",
      "CLEANED M linear\n",
      "first part which\n",
      "CLEANED M understanding\n",
      "first part for\n",
      "CLEANED M does\n",
      "first part that\n",
      "first part model\n",
      "CLEANED M model\n",
      "first part oracle\n",
      "CLEANED M penalized\n",
      "CLEANED M generality\n",
      "first part of\n",
      "first part any\n",
      "CLEANED M the\n",
      "first part where\n",
      "CLEANED M intercept\n",
      "first part the\n",
      "CLEANED M and\n",
      "CLEANED M can\n",
      "first part components\n",
      "first part mi\n",
      "CLEANED M missing\n",
      "first part have\n",
      "CLEANED M the\n",
      "first part for\n",
      "CLEANED M and\n",
      "CLEANED M observed\n",
      "first part fully\n",
      "first part to\n",
      "CLEANED M can\n",
      "first part missing\n",
      "CLEANED M have\n",
      "first part may\n",
      "CLEANED M fit\n",
      "first part to\n",
      "first part estimator\n",
      "CLEANED M the\n",
      "first part under\n",
      "CLEANED M alleviate\n",
      "first part to\n",
      "CLEANED M data\n",
      "CLEANED M weights\n",
      "first part different\n",
      "first part weighting\n",
      "CLEANED M observed\n",
      "first part fully\n",
      "CLEANED M given\n",
      "first part is\n",
      "CLEANED M with\n",
      "first part observations\n",
      "CLEANED M the\n",
      "first part estimating\n",
      "CLEANED M mean\n",
      "first part linear\n",
      "first part estimator\n",
      "CLEANED M variable\n",
      "first part bandwidth\n",
      "CLEANED M for\n",
      "first part but\n",
      "CLEANED M denote\n",
      "first part will\n",
      "CLEANED M denote\n",
      "first part will\n",
      "CLEANED M denote\n",
      "first part will\n",
      "CLEANED M weighted\n",
      "first part parametric\n",
      "CLEANED M local\n",
      "first part results\n",
      "CLEANED M requires\n",
      "first part terms\n",
      "CLEANED M element\n",
      "first part the\n",
      "first part of\n",
      "CLEANED M distribution\n",
      "first part asymptotic\n",
      "begins with n neighborhood\n",
      "first part and\n",
      "CLEANED M with\n",
      "CLEANED M that\n",
      "CLEANED M bounded\n",
      "first part have\n",
      "first part linear\n",
      "CLEANED M all\n",
      "first part that\n",
      "first part allows\n",
      "begins with n necessary\n",
      "first part is\n",
      "CLEANED M data\n",
      "first part missing\n",
      "CLEANED M the\n",
      "first part as\n",
      "CLEANED M asymptotically\n",
      "first part are\n",
      "CLEANED M the\n",
      "first part of\n",
      "CLEANED M and\n",
      "first part estimators\n",
      "first part and\n",
      "CLEANED M and\n",
      "first part the\n",
      "CLEANED M and\n",
      "first part define\n",
      "begins with n ne\n",
      "first part the\n",
      "CLEANED M weights\n",
      "first part true\n",
      "first part of\n",
      "CLEANED M from\n",
      "first part results\n",
      "first part and\n",
      "first part if\n",
      "CLEANED M found\n",
      "first part results\n",
      "CLEANED M why\n",
      "first part outlines\n",
      "first part model\n",
      "CLEANED M distribution\n",
      "first part the\n",
      "first part is\n",
      "CLEANED M are\n",
      "first part that\n",
      "CLEANED M same\n",
      "first part the\n",
      "CLEANED M and\n",
      "first part terms\n",
      "CLEANED M location\n",
      "first part the\n",
      "CLEANED M under\n",
      "first part hold\n",
      "CLEANED M under\n",
      "CLEANED M case\n",
      "first part dimensional\n",
      "CLEANED M missing\n",
      "first part with\n",
      "first part model\n",
      "first part be\n",
      "CLEANED M have\n",
      "first part would\n",
      "CLEANED M have\n",
      "first part and\n",
      "CLEANED M the\n",
      "first part for\n",
      "CLEANED M setting\n",
      "first part dimensional\n",
      "first part one\n",
      "CLEANED M the\n",
      "first part for\n",
      "CLEANED M the\n",
      "first part for\n",
      "first part rate\n",
      "first part max\n",
      "CLEANED M dimensional\n",
      "first part fixed\n",
      "first part mentioned\n",
      "CLEANED M dimensions\n",
      "first part fixed\n",
      "CLEANED M more\n",
      "first part much\n",
      "CLEANED M knowledge\n",
      "CLEANED M the\n",
      "first part of\n",
      "CLEANED M the\n",
      "first part with\n",
      "CLEANED M could\n",
      "first part penalty\n",
      "CLEANED M require\n",
      "first part would\n",
      "CLEANED M was\n",
      "first part method\n",
      "CLEANED M bias\n",
      "first part alleviate\n",
      "CLEANED M estimate\n",
      "first part rigorously\n",
      "CLEANED M data\n",
      "first part missing\n",
      "CLEANED M consistency\n",
      "first part proposed\n",
      "first part form\n",
      "first part for\n",
      "CLEANED M they\n",
      "first part is\n",
      "first part zero\n",
      "first part and\n",
      "CLEANED M functions\n",
      "first part penalty\n",
      "CLEANED M and\n",
      "first part model\n",
      "CLEANED M figure\n",
      "CLEANED M the\n",
      "first part therefore\n",
      "CLEANED M high\n",
      "first part the\n",
      "first part quickly\n",
      "first part and\n",
      "CLEANED M equivalent\n",
      "first part are\n",
      "CLEANED M function\n",
      "first part penalty\n",
      "CLEANED M weights\n",
      "first part parametric\n",
      "first part let\n",
      "begins with n no\n",
      "first part the\n",
      "first part let\n",
      "CLEANED M kernel\n",
      "first part the\n",
      "first part max\n",
      "first part if\n",
      "first part let\n",
      "first part and\n",
      "CLEANED M conditions\n",
      "first part the\n",
      "CLEANED M the\n",
      "first part of\n",
      "first part stated\n",
      "CLEANED M weights\n",
      "first part nonparametric\n",
      "CLEANED M sufficient\n",
      "first part present\n",
      "CLEANED M the\n",
      "first part asymptotically\n",
      "first part and\n",
      "CLEANED M the\n",
      "first part estimator\n",
      "CLEANED M approximation\n",
      "first part linear\n",
      "CLEANED M close\n",
      "first part sufficiently\n",
      "CLEANED M penalty\n",
      "first part lasso\n",
      "CLEANED M adapted\n",
      "first part is\n",
      "CLEANED M monte\n",
      "first part results\n",
      "CLEANED M performance\n",
      "first part size\n",
      "first part of\n",
      "CLEANED M linear\n",
      "first part the\n",
      "CLEANED M mechanism\n",
      "first part generating\n",
      "first part is\n",
      "CLEANED M are\n",
      "first part there\n",
      "first part used\n",
      "CLEANED M the\n",
      "first part influence\n",
      "CLEANED M variables\n",
      "first part the\n",
      "first part binomial\n",
      "first part after\n",
      "CLEANED M model\n",
      "first part final\n",
      "first part variable\n",
      "CLEANED M weights\n",
      "first part estimated\n",
      "CLEANED M being\n",
      "first part weight\n",
      "CLEANED M relates\n",
      "first part and\n",
      "CLEANED M benefits\n",
      "first part the\n",
      "CLEANED M reported\n",
      "first part are\n",
      "CLEANED M that\n",
      "first part estimator\n",
      "CLEANED M and\n",
      "first part lasso\n",
      "CLEANED M and\n",
      "first part model\n",
      "CLEANED M indexes\n",
      "CLEANED M internal\n",
      "first part of\n",
      "first part and\n",
      "first part minimizing\n",
      "CLEANED M regression\n",
      "CLEANED M fan\n",
      "first part by\n",
      "CLEANED M from\n",
      "first part using\n",
      "first part of\n",
      "CLEANED M for\n",
      "first part results\n",
      "CLEANED M there\n",
      "first part that\n",
      "CLEANED M the\n",
      "first part reduce\n",
      "CLEANED M holds\n",
      "first part model\n",
      "CLEANED M corresponding\n",
      "first part simulations\n",
      "CLEANED M estimates\n",
      "first part based\n",
      "CLEANED M model\n",
      "first part parametric\n",
      "CLEANED M that\n",
      "first part suggesting\n",
      "CLEANED M method\n",
      "first part parametric\n",
      "first part model\n",
      "CLEANED M coefficients\n",
      "first part estimated\n",
      "CLEANED M scad\n",
      "CLEANED M the\n",
      "first part analysis\n",
      "CLEANED M rehabilitation\n",
      "CLEANED M scad\n",
      "CLEANED M randomized\n",
      "first part and\n",
      "CLEANED M length\n",
      "first part and\n",
      "CLEANED M and\n",
      "first part use\n",
      "first part length\n",
      "CLEANED M predictors\n",
      "first part the\n",
      "CLEANED M shows\n",
      "first part which\n",
      "first part used\n",
      "CLEANED M regression\n",
      "first part as\n",
      "CLEANED M scad\n",
      "first part if\n",
      "CLEANED M considered\n",
      "first part are\n",
      "CLEANED M predictor\n",
      "first part only\n",
      "CLEANED M models\n",
      "first part nonlinear\n",
      "first part model\n",
      "first part as\n",
      "CLEANED M conditionally\n",
      "first part be\n",
      "CLEANED M the\n",
      "first part using\n",
      "CLEANED M the\n",
      "first part is\n",
      "CLEANED M scad\n",
      "CLEANED M partitioned\n",
      "CLEANED M with\n",
      "first part set\n",
      "CLEANED M training\n",
      "first part the\n",
      "CLEANED M approach\n",
      "CLEANED M penalized\n",
      "first part scad\n",
      "CLEANED M weights\n",
      "first part nonparametric\n",
      "first part estimate\n",
      "CLEANED M prediction\n",
      "first part average\n",
      "CLEANED M close\n",
      "first part are\n",
      "CLEANED M lengths\n",
      "first part interval\n",
      "CLEANED M the\n",
      "first part after\n",
      "CLEANED M outperforming\n",
      "first part dramatically\n",
      "CLEANED M point\n",
      "first part last\n",
      "CLEANED M when\n",
      "first part variable\n",
      "CLEANED M drastically\n",
      "CLEANED M ignoring\n",
      "first part if\n",
      "CLEANED M relationships\n",
      "first part nonlinear\n",
      "first part missing\n",
      "CLEANED M ensures\n",
      "first part this\n",
      "CLEANED M have\n",
      "first part variables\n",
      "CLEANED M will\n",
      "first part used\n",
      "CLEANED M and\n",
      "first part simulations\n",
      "CLEANED M presence\n",
      "first part the\n",
      "CLEANED M handling\n",
      "first part for\n",
      "first part proposed\n",
      "first part and\n",
      "CLEANED M including\n",
      "first part by\n",
      "CLEANED M variable\n",
      "first part original\n",
      "CLEANED M terms\n",
      "first part nonlinear\n",
      "CLEANED M the\n",
      "first part for\n",
      "first part area\n",
      "first part appendix\n",
      "CLEANED M throughout\n",
      "first part functions\n",
      "CLEANED M proofs\n",
      "first part parametric\n",
      "CLEANED M kernel\n",
      "first part the\n",
      "CLEANED M common\n",
      "first part the\n",
      "CLEANED M typically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first part proofs\n",
      "CLEANED M this\n",
      "first part of\n",
      "CLEANED M constant\n",
      "first part positive\n",
      "first part defining\n",
      "begins with n ne\n",
      "CLEANED M basis\n",
      "first part of\n",
      "CLEANED M unknown\n",
      "first part the\n",
      "CLEANED M growing\n",
      "first part the\n",
      "CLEANED M provides\n",
      "first part which\n",
      "first part sup\n",
      "begins with n ne\n",
      "CLEANED M large\n",
      "first part sufficiently\n",
      "CLEANED M large\n",
      "first part sufficiently\n",
      "CLEANED M condition\n",
      "first part that\n",
      "CLEANED M uniform\n",
      "first part the\n",
      "CLEANED M weights\n",
      "first part estimated\n",
      "CLEANED M material\n",
      "first part supplementary\n",
      "first part from\n",
      "CLEANED M with\n",
      "first part this\n",
      "CLEANED M bound\n",
      "first part upper\n",
      "CLEANED M rate\n",
      "first part the\n",
      "first part case\n",
      "CLEANED M material\n",
      "first part supplementary\n",
      "first part second\n",
      "begins with n ne\n",
      "CLEANED M variance\n",
      "first part the\n",
      "first part is\n",
      "first part is\n",
      "begins with n ne\n",
      "begins with n ne\n",
      "CLEANED M the\n",
      "first part and\n",
      "first part get\n",
      "begins with n ne\n",
      "begins with n ne\n",
      "first part model\n",
      "CLEANED M for\n",
      "first part components\n",
      "first part by\n",
      "CLEANED M sufficient\n",
      "first part is\n",
      "first part one\n",
      "CLEANED M from\n",
      "first part holds\n",
      "CLEANED M theorem\n",
      "first part of\n",
      "first part show\n",
      "first part modified\n",
      "first part show\n",
      "first part max\n",
      "CLEANED M the\n",
      "first part under\n",
      "begins with n no\n",
      "CLEANED M their\n",
      "first part for\n",
      "CLEANED M improve\n",
      "first part significantly\n",
      "CLEANED M with\n",
      "first part analysis\n",
      "CLEANED M high\n",
      "first part with\n",
      "CLEANED M and\n",
      "first part likelihood\n",
      "CLEANED M models\n",
      "first part linear\n",
      "CLEANED M model\n",
      "first part linear\n",
      "CLEANED M for\n",
      "first part model\n",
      "CLEANED M additive\n",
      "first part linear\n",
      "CLEANED M linear\n",
      "first part partially\n",
      "CLEANED M regression\n",
      "begins with n nr\n",
      "begins with n nates\n",
      "first part for\n",
      "CLEANED M when\n",
      "first part coefficients\n",
      "CLEANED M package\n",
      "CLEANED M analyzing\n",
      "first part for\n",
      "CLEANED M estimation\n",
      "CLEANED M varying\n",
      "first part linear\n",
      "CLEANED M random\n",
      "first part at\n",
      "CLEANED M dropouts\n",
      "first part with\n",
      "CLEANED M penalty\n",
      "first part concave\n",
      "CLEANED M structure\n",
      "first part automatic\n",
      "CLEANED M likelihood\n",
      "first part penalized\n",
      "begins with n ne\n",
      "first part university\n",
      "first part results\n"
     ]
    }
   ],
   "source": [
    "for i in wrong:\n",
    "    if 'nn' in i:\n",
    "        #print('THREE N-S',words_c[w])\n",
    "        j=i[i.find('nn')+1:len(i)]\n",
    "        k=j[1:]\n",
    "        l=i[:i.find('nn')]\n",
    "        if len(j) > 1 and d.check(j) == True:\n",
    "            print('begins with n',j)\n",
    "        if len(k) > 2 and d.check(k) == True:\n",
    "            print('CLEANED M',k)\n",
    "        if len(l) > 1 and d.check(l) == True:\n",
    "            print('first part',l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\n",
      "first part regression\n",
      "linear\n",
      "first part introduction\n",
      "because\n",
      "first part regression\n",
      "has\n",
      "first part observation\n",
      "begins with n nonlinear\n",
      "first part unknown\n",
      "begins with n nonzero\n",
      "first part known\n",
      "section\n",
      "first part in\n",
      "this\n",
      "first part regression\n",
      "begins with n nonlinear\n",
      "first part function\n",
      "begins with n next\n",
      "ext\n",
      "other\n",
      "first part in\n",
      "indicator\n",
      "first part an\n",
      "begins with n no\n",
      "first part regression\n",
      "estimator\n",
      "first part regression\n",
      "detail\n",
      "first part in\n",
      "first part ton\n",
      "models\n",
      "first part mean\n",
      "models\n",
      "first part regression\n",
      "scale\n",
      "first part location\n",
      "begins with n not\n",
      "first part selection\n",
      "first part function\n",
      "first part can\n",
      "considering\n",
      "first part when\n",
      "and\n",
      "first part fan\n",
      "begins with n not\n",
      "first part don\n",
      "first part then\n",
      "first part min\n",
      "first part then\n",
      "first part can\n",
      "first part an\n",
      "first part when\n",
      "begins with n number\n",
      "umber\n",
      "and\n",
      "first part ln\n",
      "with\n",
      "first part regression\n",
      "first part ln\n",
      "begins with n not\n",
      "begins with n none\n",
      "one\n",
      "begins with n not\n",
      "first part deviation\n",
      "begins with n naive\n",
      "this\n",
      "first part discussion\n",
      "and\n",
      "first part estimation\n",
      "methods\n",
      "first part driven\n",
      "first part then\n",
      "begins with n not\n",
      "begins with n norm\n",
      "begins with n nonlinear\n",
      "first part then\n",
      "begins with n notice\n",
      "first part then\n",
      "first part an\n",
      "first part function\n",
      "first part then\n",
      "theorem\n",
      "first part in\n",
      "first part then\n",
      "modeling\n",
      "first part regression\n",
      "begins with n nonparametric\n",
      "first part function\n"
     ]
    }
   ],
   "source": [
    "for i in ns:\n",
    "    j=i[i.find('nn')+2:len(i)]\n",
    "    k=j[1:]\n",
    "    l=i[:i.find('nn')+1]\n",
    "    if len(j) > 1 and d.check(j) == True:\n",
    "        print('begins with n',j)\n",
    "    if len(k) > 2 and d.check(k) == True:\n",
    "        print(k)\n",
    "    if len(l) > 1 and d.check(l) == True:\n",
    "        print('first part',l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quantile', 'regression'),\n",
       " ('additive', 'partial'),\n",
       " ('partial', 'linear'),\n",
       " ('linear', 'quantile'),\n",
       " ('selection', 'additive'),\n",
       " ('missing', 'covariates'),\n",
       " ('scad', 'wt'),\n",
       " ('regression', 'missing'),\n",
       " ('et', 'al'),\n",
       " ('objective', 'function'),\n",
       " ('wt', 'scad'),\n",
       " ('high', 'dimensional'),\n",
       " ('missing', 'data'),\n",
       " ('amer', 'statist'),\n",
       " ('chen', 'wan')]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.corpus\n",
    "from nltk.text import Text\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "\n",
    "biagram_collocation = BigramCollocationFinder.from_words(words_cleaned)\n",
    "biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 15) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(path+'0.00094_cleaned.txt','w') \n",
    "\n",
    "file.write(str(words_cleaned))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(path+'0.00094_wrong.txt','w') \n",
    "\n",
    "file.write(str(wrong))\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
