{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaner for arxiv papers\n",
    "def arxiv_cleaner(dir,limit):\n",
    "    import nltk.corpus  \n",
    "    from nltk import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    import enchant\n",
    "    import os\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    \n",
    "    stop=0\n",
    "    counter=0\n",
    "    for root,dirs,files, in os.walk(dir):\n",
    "        for file in files:\n",
    "            stop+=1\n",
    "            if stop == limit:\n",
    "                break\n",
    "            path_to_txt = os.path.join(root, file)\n",
    "            [stem, ext] = os.path.splitext(path_to_txt)\n",
    "            if ext == '.txt':\n",
    "                paper = open(stem+ext,'r')\n",
    "                paper_token=word_tokenize(paper.read()) \n",
    "                #correction of words broken into two parts\n",
    "                words_c = paper_token\n",
    "                words_cleaned = []\n",
    "                wrong=[]\n",
    "                n=0\n",
    "                counter+=1\n",
    "\n",
    "                for w in range(len(words_c)):\n",
    "                    n+=1\n",
    "                    if 'nnn' in words_c[w]:\n",
    "                        j=words_c[w][words_c[w].find('nn')+2:len(words_c[w])]\n",
    "                        k=j[1:]\n",
    "                        l=words_c[w][:words_c[w].find('nn')+1]\n",
    "                        if len(j) > 1 and d.check(j) == True:\n",
    "                            words_cleaned.append(j)\n",
    "                        if len(k) > 2 and d.check(k) == True:\n",
    "                            words_cleaned.append(k)\n",
    "                        if len(l) > 1 and d.check(l) == True:\n",
    "                            words_cleaned.append(l)\n",
    "                    elif 'nn' in words_c[w]:\n",
    "                        if words_c[w].startswith('nn'):\n",
    "                            clean=words_c[w].replace('nn','')\n",
    "                            clean1=words_c[w-1]+clean\n",
    "                            if d.check(clean1) == True:\n",
    "                                words_cleaned.append(clean1)\n",
    "                            elif len(clean) > 1 and d.check(clean) == True:\n",
    "                                words_cleaned.append(clean)\n",
    "                            else:\n",
    "                                continue\n",
    "                        elif words_c[w].endswith('nn'):\n",
    "                            clean=words_c[w].replace('nn','')\n",
    "                            if n < len(words_c)-1:\n",
    "                                clean1=clean+words_c[w+1]\n",
    "                            else:\n",
    "                                clean1=clean+words_c[w]\n",
    "                            if d.check(clean1) == True:\n",
    "                                words_cleaned.append(clean1)\n",
    "                            elif len(clean) > 1 and d.check(clean) == True:\n",
    "                                words_cleaned.append(clean)\n",
    "                            else:\n",
    "                                wrong.append(words_c[w])\n",
    "                        else:\n",
    "                            j=words_c[w][words_c[w].find('nn')+1:len(words_c[w])]\n",
    "                            k=j[1:]\n",
    "                            l=words_c[w][:words_c[w].find('nn')]\n",
    "                            if len(j) > 1 and d.check(j) == True:\n",
    "                                words_cleaned.append(j)\n",
    "                            if len(k) > 2 and d.check(k) == True:\n",
    "                                words_cleaned.append(k)\n",
    "                            if len(l) > 1 and d.check(l) == True:\n",
    "                                words_cleaned.append(l)\n",
    "                    elif (len(words_c[w]) > 2) and (d.check(words_c[w]) == True):\n",
    "                        words_cleaned.append(words_c[w])\n",
    "                    elif len(words_c[w]) > 2:\n",
    "                        words_cleaned.append(words_c[w])\n",
    "                    elif words_c[w] == \"em\":\n",
    "                        words_cleaned.append(words_c[w])\n",
    "                    else:    \n",
    "                        wrong.append(words_c[w])\n",
    "                        continue\n",
    "\n",
    "                #removing stopwords\n",
    "                words_cleaned_1 = [w for w in words_cleaned if not w in stop_words]\n",
    "\n",
    "                #cleaning from nonsense 'x'-expressions (equations)\n",
    "                words_cleaned_2 = [w for w in words_cleaned_1 if not w.startswith('x') and not w.endswith('x') and len(w) > 2]\n",
    "\n",
    "                path_to_cleaned=stem+'_cleaned'+ext\n",
    "                with open(path_to_cleaned, 'w') as cleaned:\n",
    "                    #print(\"Writing contents to \" + path_to_cleaned)\n",
    "                    print('paper #',counter)\n",
    "                    cleaned.write(str(words_cleaned_2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}