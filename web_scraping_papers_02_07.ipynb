{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google scholar scrape v1\n",
    "\n",
    "# https://realpython.com/beautiful-soup-web-scraper-python/\n",
    "# https://requests.readthedocs.io/en/master/user/quickstart/#custom-headers\n",
    "\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "URL = 'https://scholar.google.com/scholar?hl=hu&as_sdt=0%2C5&as_ylo=2019&as_yhi=2019&q=%22missing+data%22&btnG='\n",
    "url2 = 'https://scholar.google.com/scholar?start=10&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019'\n",
    "page = requests.get(url2)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "#for link in soup.find_all('a'):\n",
    "#    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=10&amp;d=1267018382220661572&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm1XjEX0q31EW08B5PbC4B0uiZdwkg&amp;nossl=1\" data-clk-atid=\"RBMndIFalREJ\" href=\"https://link.springer.com/chapter/10.1007/978-3-030-16621-2_13\" id=\"RBMndIFalREJ\">\n",
      " <b>\n",
      "  Missing data\n",
      " </b>\n",
      " imputation through SGTM neural-like structure for environmental monitoring tasks\n",
      "</a>\n",
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=11&amp;d=13804137567488879597&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm26jriVzTGrOMGh_A7u4g8pjsplfg&amp;nossl=1\" data-clk-atid=\"7avM7Polkr8J\" href=\"https://ieeexplore.ieee.org/abstract/document/8624600/\" id=\"7avM7Polkr8J\">\n",
      " Fast matrix factorization with nonuniform weights on\n",
      " <b>\n",
      "  missing data\n",
      " </b>\n",
      "</a>\n",
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=12&amp;d=10481738642016543521&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm0g8wAB4Xyxi1be6EJ7FUHztZGTug&amp;nossl=1\" data-clk-atid=\"IaPhocKddpEJ\" href=\"https://journals.sagepub.com/doi/abs/10.1177/1460458217733288\" id=\"IaPhocKddpEJ\">\n",
      " Challenges associated with\n",
      " <b>\n",
      "  missing data\n",
      " </b>\n",
      " in electronic health records: a case study of a risk prediction model for diabetes using data from Slovenian primary care\n",
      "</a>\n",
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=13&amp;d=1350929034423412009&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm2xf6PdAMO4atTUhFfOyBGhBHQoFQ&amp;nossl=1\" data-clk-atid=\"KaWRncx2vxIJ\" href=\"https://link.springer.com/chapter/10.1007/978-3-030-20521-8_64\" id=\"KaWRncx2vxIJ\">\n",
      " SGD-based Wiener Polynomial Approximation for\n",
      " <b>\n",
      "  Missing Data\n",
      " </b>\n",
      " Recovery in Air Pollution Monitoring Dataset\n",
      "</a>\n",
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=14&amp;d=317623883745549921&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm3vA4N7PekJQBWnTv9aA0q96FsXoQ&amp;nossl=1\" data-clk-atid=\"YV7VSj1taAQJ\" href=\"https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12326\" id=\"YV7VSj1taAQJ\">\n",
      " High dimensional linear discriminant analysis: optimality, adaptive algorithm and\n",
      " <b>\n",
      "  missing data\n",
      " </b>\n",
      "</a>\n",
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=15&amp;d=6531031837791360564&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm3DtCqPp4gWdCqcq5B016sqTaA5kg&amp;nossl=1\" data-clk-atid=\"NBZShNrioloJ\" href=\"http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_CollaGAN_Collaborative_GAN_for_Missing_Image_Data_Imputation_CVPR_2019_paper.html\" id=\"NBZShNrioloJ\">\n",
      " CollaGAN: Collaborative GAN for missing image data imputation\n",
      "</a>\n",
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=16&amp;d=907365870543295359&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm3AAFdouFbzDvm0nD4onPb7gdiLig&amp;nossl=1\" data-clk-atid=\"f9MLq3OclwwJ\" href=\"https://www.tandfonline.com/doi/abs/10.1080/00223891.2018.1530680\" id=\"f9MLq3OclwwJ\">\n",
      " Rebutting existing misconceptions about multiple imputation as a method for handling\n",
      " <b>\n",
      "  missing data\n",
      " </b>\n",
      "</a>\n",
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=17&amp;d=10375261307223253508&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm1ab-uCurdomYgHMImfG0p34eeR2g&amp;nossl=1\" data-clk-atid=\"BKLlPDBV_I8J\" href=\"https://www.igi-global.com/chapter/data-imputation-methods-for-missing-values-in-the-context-of-clustering/220793\" id=\"BKLlPDBV_I8J\">\n",
      " Data Imputation Methods for Missing Values in the Context of Clustering\n",
      "</a>\n",
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=18&amp;d=5103934375454566274&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm2NKDRu1m3fh2_jKXI3lKHrZUGRuA&amp;nossl=1\" data-clk-atid=\"guffzUXR1EYJ\" href=\"https://journals.sagepub.com/doi/abs/10.1177/0962280217722382\" id=\"guffzUXR1EYJ\">\n",
      " Simulation-based sensitivity analysis for non-ignorably\n",
      " <b>\n",
      "  missing data\n",
      " </b>\n",
      "</a>\n",
      "<a data-clk=\"hl=hu&amp;sa=T&amp;ct=res&amp;cd=19&amp;d=6880455277967163665&amp;ei=phU4XqeZLYr5mQHk-oaoAQ&amp;scisig=AAGBfm0T1R0a7tNIcZj6rm03p7NKCNI0xw&amp;nossl=1\" data-clk-atid=\"EcExAJ9JfF8J\" href=\"https://www.sciencedirect.com/science/article/pii/S0167947318301944\" id=\"EcExAJ9JfF8J\">\n",
      " Partially observed functional data: The case of systematically missing parts\n",
      "</a>\n"
     ]
    }
   ],
   "source": [
    "papers = soup.find_all('h3', class_='gs_rt')\n",
    "\n",
    "\n",
    "papers_list = []\n",
    "\n",
    "\n",
    "for i in papers:\n",
    "    children = i.findChildren(\"a\" , recursive=False)\n",
    "    #print(children)\n",
    "    for child in children:\n",
    "        print(child.prettify())\n",
    "        papers_list.append(child.prettify())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TITLE------\n",
      "\n",
      "  \n",
      "  Missing data\n",
      "  \n",
      " imputation through SGTM neural-like structure for environmental monitoring tasks\n",
      "\n",
      "-----TITLE------\n",
      "\n",
      " Fast matrix factorization with nonuniform weights on\n",
      "  \n",
      "  missing data\n",
      "  \n",
      "\n",
      "-----TITLE------\n",
      "\n",
      " Challenges associated with\n",
      "  \n",
      "  missing data\n",
      "  \n",
      " in electronic health records: a case study of a risk prediction model for diabetes using data from Slovenian primary care\n",
      "\n",
      "-----TITLE------\n",
      "\n",
      " SGD-based Wiener Polynomial Approximation for\n",
      "  \n",
      "  Missing Data\n",
      "  \n",
      " Recovery in Air Pollution Monitoring Dataset\n",
      "\n",
      "-----TITLE------\n",
      "\n",
      " High dimensional linear discriminant analysis: optimality, adaptive algorithm and\n",
      "  \n",
      "  missing data\n",
      "  \n",
      "\n",
      "-----TITLE------\n",
      "\n",
      " CollaGAN: Collaborative GAN for missing image data imputation\n",
      "\n",
      "-----TITLE------\n",
      "\n",
      " Rebutting existing misconceptions about multiple imputation as a method for handling\n",
      "  \n",
      "  missing data\n",
      "  \n",
      "\n",
      "-----TITLE------\n",
      "\n",
      " Data Imputation Methods for Missing Values in the Context of Clustering\n",
      "\n",
      "-----TITLE------\n",
      "\n",
      " Simulation-based sensitivity analysis for non-ignorably\n",
      "  \n",
      "  missing data\n",
      "  \n",
      "\n",
      "-----TITLE------\n",
      "\n",
      " Partially observed functional data: The case of systematically missing parts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "links_list = []\n",
    "titles = []\n",
    "\n",
    "for paper in papers_list:\n",
    "    #print('--------------PAPER---------------')\n",
    "    #print(paper[paper.find('data-clk-atid'):])\n",
    "    #print(paper[paper.find('href=\"')+6:paper.find('>')-19])\n",
    "    links_list.append(paper[paper.find('href=\"')+6:paper.find('>')-19])\n",
    "    titles.append(paper[paper.find('data-clk-atid='):paper.find('</a>')])\n",
    "\n",
    "\n",
    "#links_list\n",
    "for i in titles:\n",
    "    print('-----TITLE------')\n",
    "    print(i[i.find('\">')+2:].replace('</b>',' ').replace('<b>',' '))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# navigating on website\n",
    "# https://stackoverflow.com/questions/46753393/how-to-make-firefox-headless-programmatically-in-selenium-with-python\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "options = FirefoxOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "\n",
    "browser = webdriver.Firefox(options=options)\n",
    "browser.get(URL)\n",
    "\n",
    "html_source = browser.page_source\n",
    "\n",
    "# combining with BS\n",
    "pages_nav = []\n",
    "soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    if '/scholar?start=' in link.get('href'):\n",
    "        pages_nav.append(link.get('href'))\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    #print(link.get('href'))\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "#print(html_source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scholar?start=10&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=20&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=30&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=40&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=50&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=60&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=70&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=80&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=90&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=10&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=10&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=20&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=30&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=40&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=50&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=60&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=70&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=80&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',\n",
       " '/scholar?start=90&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 10 pages -> 100 papers\n",
    "pages_nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [429]>\n",
      "Too many requests!(page:1)\n",
      "<Response [429]>\n",
      "Too many requests!(page:2)\n",
      "<Response [429]>\n",
      "Too many requests!(page:3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-64ee725faf64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;31m#print(child.prettify())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mpapers_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over pages\n",
    "# https://2019.www.torproject.org/docs/debian.html.en\n",
    "# https://medium.com/@jasonrigden/using-tor-with-the-python-request-library-79015b2606cb\n",
    "\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint \n",
    "import tor\n",
    "\n",
    "papers_list = []\n",
    "\n",
    "\n",
    "headers = {'User-Agent': \n",
    "           'Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0'}\n",
    "\n",
    "for page_num in range(10,1010,10):\n",
    "    # need to fix tor session\n",
    "    \n",
    "    #session = requests.session()\n",
    "    #session.proxies = {}\n",
    "\n",
    "    #session.proxies['http'] = 'forward-socks4a / localhost:9050'\n",
    "    #session.proxies['https'] = 'forward-socks4a / localhost:9050'\n",
    "    #page = session.get('https://scholar.google.com/scholar?start='+str(page_num)\n",
    "     #                   +'&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',headers=headers)\n",
    "    page = requests.get('https://scholar.google.com/scholar?start='+str(page_num)\n",
    "                        +'&q=%22missing+data%22&hl=hu&as_sdt=0,5&as_ylo=2019&as_yhi=2019',headers=headers)\n",
    "    print(page)\n",
    "    if page.status_code == 429:\n",
    "        print('Too many requests!(page:'+str(int(page_num/10))+')')  \n",
    "    elif page.status_code == 200:\n",
    "        print('Success!(page'+str(int(page_num/10))+')')        \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    papers = soup.find_all('h3', class_='gs_rt')\n",
    "\n",
    "    for i in papers:\n",
    "        children = i.findChildren(\"a\" , recursive=False)\n",
    "        #print(children)\n",
    "        for child in children:\n",
    "            #print(child.prettify())\n",
    "            papers_list.append(child.prettify())\n",
    "    time.sleep(randint(0,3))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in papers:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links_list = []\n",
    "titles = []\n",
    "titles_clean = []\n",
    "\n",
    "for paper in papers_list:\n",
    "    #print('--------------PAPER---------------')\n",
    "    #print(paper[paper.find('data-clk-atid'):])\n",
    "    #print(paper[paper.find('href=\"')+6:paper.find('>')-19])\n",
    "    links_list.append(paper[paper.find('href=\"')+6:paper.find('>')-19])\n",
    "    titles.append(paper[paper.find('data-clk-atid='):paper.find('</a>')])\n",
    "\n",
    "\n",
    "#links_list\n",
    "for i in titles:\n",
    "    #print('-----TITLE------')\n",
    "    print(i[i.find('\">')+2:].replace('</b>',' ').replace('<b>',' '))\n",
    "    titles_clean.append(i[i.find('\">')+2:].replace('</b>',' ').replace('<b>',' ').replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [paper title, links of papers]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = \"c:/Users/soirk/Krisztian/Egyetem/Survey Statisztika Msc/Szakdolgozat/txts/\"\n",
    "\n",
    "papers = pd.concat([pd.DataFrame(titles_clean, columns= ['paper title']),pd.DataFrame(links_list, columns=['links of papers'])], axis=1)\n",
    "\n",
    "print(papers)\n",
    "\n",
    "papers.to_csv(path +'paper_links2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
