{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = 'c:/Users/soirk/Krisztian/Egyetem/Survey Statisztika Msc/Szakdolgozat/corpus/'\n",
    "\n",
    "papers_a= pd.read_csv(path+'arxiv.csv')\n",
    "papers_j= pd.read_csv(path+'jstor_metadata.csv')\n",
    "\n",
    "papers_j=papers_j.drop(columns=['journal_title'])\n",
    "papers_j.columns = ['id','title','date','category']\n",
    "\n",
    "papers=pd.concat([papers_a,papers_j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "titles_token = []\n",
    "for title in papers['title']:\n",
    "    titles_token.append(word_tokenize(str(title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "titles_stemmed=[]\n",
    "\n",
    "for i in range(len(titles_token)):\n",
    "    words_stemmed=[]\n",
    "    for words in titles_token[i]:\n",
    "        words_stemmed.append(ps.stem(words))\n",
    "    titles_stemmed.append(words_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "title_fdist=[]\n",
    "\n",
    "miss=0\n",
    "data=0\n",
    "handl=0\n",
    "obs=0\n",
    "method=0\n",
    "incomp=0\n",
    "\n",
    "for title in titles_stemmed:\n",
    "    title_fdist.append(dict(FreqDist(title)))\n",
    "    if 'miss' in dict(FreqDist(title)):\n",
    "        miss+=1\n",
    "    elif 'handl' in dict(FreqDist(title)):\n",
    "        handl+=1\n",
    "    elif 'data' in dict(FreqDist(title)):\n",
    "        data+=1\n",
    "    elif 'observ' in dict(FreqDist(title)):\n",
    "        obs+=1\n",
    "    elif 'method' in dict(FreqDist(title)):\n",
    "        method+=1\n",
    "    elif 'incomplet' in dict(FreqDist(title)):\n",
    "        incomp+=1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "papers['title']=titles_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighter(word_names,word_list):\n",
    "    weight_list=[]\n",
    "    for name,word in zip(word_names,word_list):\n",
    "        weight_list.append([name,round(1/word*100,3)])\n",
    "    return(weight_list)\n",
    "\n",
    "\n",
    "words=[miss,data,handl,obs,method,incomp]\n",
    "names=['miss','data','handl','obs','method','incomp']\n",
    "\n",
    "weights=weighter(names,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "miss_list=[]\n",
    "data_list=[]\n",
    "handl_list=[]\n",
    "obs_list=[]\n",
    "method_list=[]\n",
    "incomp_list=[]\n",
    "\n",
    "\n",
    "for title in papers['title']:\n",
    "    f=FreqDist(title)\n",
    "    if 'miss' in f:\n",
    "        miss_list.append(f['miss']*weights[0][1])\n",
    "    else:\n",
    "        miss_list.append(0)\n",
    "    if 'data' in f:\n",
    "        data_list.append(f['data']*weights[1][1])\n",
    "    else:\n",
    "        data_list.append(0)\n",
    "    if 'handl' in f:\n",
    "        handl_list.append(f['handl']*weights[2][1])\n",
    "    else:\n",
    "        handl_list.append(0)\n",
    "    if 'observ' in f:\n",
    "        obs_list.append(f['observ']*weights[3][1])\n",
    "    else:\n",
    "        obs_list.append(0)\n",
    "    if 'method' in f:\n",
    "        method_list.append(f['method']*weights[4][1])\n",
    "    else:\n",
    "        method_list.append(0)\n",
    "    if 'incomplet' in f:\n",
    "        incomp_list.append(f['incomplet']*weights[5][1])\n",
    "    else:\n",
    "        incomp_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weights=pd.DataFrame(list(zip(miss_list,data_list,handl_list,obs_list,method_list,incomp_list)),columns=['miss','data','handl','obs','method','incomp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weights=word_weights.reset_index(drop=True)\n",
    "papers=papers.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_w=pd.concat([papers,word_weights],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_w.to_csv(r'c:\\Users\\soirk\\Krisztian\\Egyetem\\Survey Statisztika Msc\\Szakdolgozat\\corpus\\papers_w.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}