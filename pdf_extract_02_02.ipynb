{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# converts .pdf files in selected directory to .txt files\n",
    "\n",
    "from tika import parser\n",
    "import os\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# https://stackoverflow.com/questions/34837707/how-to-extract-text-from-a-pdf-file \n",
    "\n",
    "def extract_text_from_pdfs_recursively(dir):\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            path_to_pdf = os.path.join(root, file)\n",
    "            [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "            #s_words = set(stopwords.words('english'))\n",
    "            #print(s_words)\n",
    "            if ext == '.pdf':\n",
    "                #print(\"Processing \" + path_to_pdf)\n",
    "                pdf_contents = parser.from_file(path_to_pdf)\n",
    "                \n",
    "                # problem: saves the txts to the same directory\n",
    "                \n",
    "                path_to_txt = stem + '.txt'\n",
    "                with open(path_to_txt, 'w') as txt_file:\n",
    "                    #print(\"Writing contents to \" + path_to_txt)\n",
    "                    \n",
    "                    # a bit ugly...\n",
    "                    \n",
    "                    pdf_contents = str(pdf_contents['content'].encode('utf-8', errors='ignore')).replace(\"\\n\", \"\").replace(\"\\\\\", \"\").lower()\n",
    "                    pdf_contents = re.sub(\"(\\\\d|\\\\W)+\",\" \",pdf_contents)\n",
    "                    \n",
    "                    \n",
    "                    #pdf_contents = word_tokenize(pdf_contents)\n",
    "                    #pdf_contents = [x for x in pdf_contents if x not in s_words]\n",
    "                    \n",
    "                    txt_file.write(str(pdf_contents))\n",
    "                    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_text_from_pdfs_recursively(\"c:/Users/soirk/Krisztian/Egyetem/Survey Statisztika Msc/Szakdolgozat/pdfs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# searching for keywords\n",
    "# ~working\n",
    "\n",
    "# https://stackoverflow.com/questions/47649987/how-to-save-nltk-concordance-results-in-a-list\n",
    "\n",
    "def concordance(ci, word, width=75, lines=100):\n",
    "    \"\"\"\n",
    "    Rewrite of nltk.text.ConcordanceIndex.print_concordance that returns results\n",
    "    instead of printing them. \n",
    "\n",
    "    See:\n",
    "    http://www.nltk.org/api/nltk.html#nltk.text.ConcordanceIndex.print_concordance\n",
    "    \"\"\"\n",
    "    half_width = (width - len(word) - 2) // 2\n",
    "    context = width // 4 # approx number of words of context\n",
    "\n",
    "    results = []\n",
    "    offsets = ci.offsets(word)\n",
    "    if offsets:\n",
    "        lines = min(lines, len(offsets))\n",
    "        for i in offsets:\n",
    "            if lines <= 0:\n",
    "                break\n",
    "            left = (' ' * half_width +\n",
    "                    ' '.join(ci._tokens[i-context:i]))\n",
    "            right = ' '.join(ci._tokens[i+1:i+context])\n",
    "            left = left[-half_width:]\n",
    "            right = right[:half_width]\n",
    "            results.append('%s %s %s' % (left, ci._tokens[i], right))\n",
    "            lines -= 1\n",
    "\n",
    "    return results\n",
    "\n",
    "# https://stackoverflow.com/questions/29110950/python-concordance-command-in-nltk\n",
    "# http://www.nltk.org/book/ch03.html\n",
    "\n",
    "import nltk.corpus  \n",
    "from nltk import word_tokenize\n",
    "from nltk.text import Text  \n",
    "from nltk import ConcordanceIndex\n",
    "\n",
    "path = 'c:/Users/soirk/Krisztian/Egyetem/Survey Statisztika Msc/Szakdolgozat/txts/'\n",
    "\n",
    "# searching for 'missing' in one paper\n",
    "\n",
    "paper = open(path+'Briggs-2003-Missing-presumed-at-random-cost-ana.txt','rb')\n",
    "\n",
    "paper_token = word_tokenize(str(paper.read()))\n",
    "\n",
    "paper_txt = Text(paper_token)\n",
    "\n",
    "\n",
    "\n",
    "# occurence of 'missing' in the text\n",
    "\n",
    "ci = ConcordanceIndex(paper_txt.tokens)\n",
    "\n",
    "paper_conc = str(concordance(ci,'missing'))\n",
    "\n",
    "paper_conc\n",
    "\n",
    "#len(paper_txt.concordance_list('missing')) # gives only the length of the displayed results\n",
    "\n",
    "#len(ConcordanceIndex(paper_txt.tokens).offsets('missing')) # gives the correct number of results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# making dataframe \n",
    "# works fine\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk.corpus  \n",
    "from nltk import word_tokenize\n",
    "from nltk.text import Text  \n",
    "from nltk import ConcordanceIndex\n",
    "\n",
    "\n",
    "path = \"c:/Users/soirk/Krisztian/Egyetem/Survey Statisztika Msc/Szakdolgozat/txts/\"\n",
    "filelist = os.listdir(path)\n",
    "filelist_sort = filelist[0:5]\n",
    "sentences_list = []\n",
    "papers_list = []\n",
    "keywords = [\"missing\",\"impute\",\"imputing\",\"imputation\",\"imputed\",\"pairwise\",\"listwise\",\"deletion\",\"delete\",\"deleted\"]\n",
    "\n",
    "papers = pd.DataFrame(filelist)\n",
    "sentences_df = pd.DataFrame(filelist)\n",
    "values_list = []\n",
    "\n",
    "for j in range(len(keywords)):\n",
    "    for i in range(len(filelist)):\n",
    "    #print(filelist[i])\n",
    "        paper = open(path+filelist[i],'rb')\n",
    "\n",
    "        paper_token = word_tokenize(str(paper.read()))\n",
    "\n",
    "        paper_txt = Text(paper_token)\n",
    "        \n",
    "        ci = ConcordanceIndex(paper_txt.tokens)\n",
    "\n",
    "        sentences = str(concordance(ci,keywords[j]))\n",
    "    \n",
    "        value = len(ConcordanceIndex(paper_txt.tokens).offsets(keywords[j]))\n",
    "        #print(keywords[j],':',value)\n",
    "        \n",
    "        og_dict = {keywords[j]:value}\n",
    "        values_list.append(value)\n",
    "        sentences_list.append(sentences)\n",
    "    \n",
    "    papers = pd.concat([papers.reset_index(drop=True), pd.DataFrame(values_list, columns = [keywords[j]])], axis=1)\n",
    "    sentences_df = pd.concat([sentences_df.reset_index(drop=True), pd.DataFrame(sentences_list, columns = [keywords[j]])], axis=1)\n",
    "    \n",
    "    \n",
    "    values_list = []\n",
    "    sentences_list = []\n",
    "\n",
    "#papers.columns = ['paper',\"missing\",\"impute\",\"imputing\",\"imputation\",\"imputed\",\"pairwise\",\"listwise\",\"deletion\",\"delete\",\"deleted\"]\n",
    "\n",
    "papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "source": [
    "papers.to_csv(path +'papers.csv')\n",
    "sentences_df.to_csv(path +'sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "og_dict\n",
    "\n",
    "og_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# dictionary\n",
    "\n",
    "class my_dictionary(dict): \n",
    "  \n",
    "    # __init__ function \n",
    "    def __init__(self): \n",
    "        self = dict() \n",
    "          \n",
    "    # Function to add key:value \n",
    "    def add(self, key, value): \n",
    "        self[key] = value \n",
    "        \n",
    "dict_obj = my_dictionary() \n",
    "  \n",
    "dict_obj.add(1, 'Geeks') \n",
    "dict_obj.add(2, 'forGeeks') \n",
    "  \n",
    "print(dict_obj) \n",
    "        \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
